{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import matplotlib\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\")\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "#plt.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions\n",
    "\n",
    "\n",
    "def find_spike_times(voltages, dt, detection_level, min_interval):\n",
    "    spike_times = []\n",
    "    min_amplitude = -10\n",
    "    last_spike_time = -min_interval\n",
    "    for i in range(1, len(voltages)):\n",
    "        t = i * dt\n",
    "        if (voltages[i - 1] < detection_level <= voltages[i]) and (voltages[i] - voltages[i - 1] >= min_amplitude) and (t - last_spike_time >= min_interval):\n",
    "            spike_times.append(t)\n",
    "            last_spike_time = t\n",
    "    return spike_times\n",
    "\n",
    "\n",
    "def group_cvs(values, group_size):\n",
    "    cvs = []\n",
    "    if len(values) >= group_size:\n",
    "        for i in range(0, len(values) - group_size, group_size):\n",
    "            mu = np.mean(values[i:i + group_size])\n",
    "            sigma = np.std(values[i:i + group_size])\n",
    "            cvs.append(sigma / mu)\n",
    "    return cvs\n",
    "\n",
    "\n",
    "def segment(values, dx, x_min, x_max):\n",
    "    return values[round(x_min / dx):round(x_max / dx)]\n",
    "\n",
    "\n",
    "def find_slopes(values, dx):\n",
    "    diffs = np.diff(values)\n",
    "    slopes = [0] * len(values)\n",
    "    slopes[0] = diffs[0] / dx\n",
    "    slopes[-1] = diffs[-1] / dx\n",
    "    for i in range(1, len(values) - 1):\n",
    "        slopes[i] = (diffs[i - 1] + diffs[i]) / (2 * dx)\n",
    "    return (slopes)\n",
    "\n",
    "\n",
    "#use neo to import either voltage or current clamp data in the correct, scaled units!\n",
    "def load_neo_file(file_name, **kwargs):\n",
    "    import neo\n",
    "    reader = neo.io.get_io(file_name)\n",
    "    blocks = reader.read(**kwargs)\n",
    "    new_blocks = []\n",
    "    for bl in blocks:\n",
    "        new_segments = []\n",
    "        for seg in bl.segments:\n",
    "            traces = []\n",
    "            count_traces = 0\n",
    "            analogsignals = seg.analogsignals\n",
    "\n",
    "            for sig in analogsignals:\n",
    "                traces.append({})\n",
    "                traces[count_traces]['T'] = sig.times.rescale('ms').magnitude\n",
    "                #need to write an if statement here for conversion\n",
    "                try:\n",
    "                    traces[count_traces]['A'] = sig.rescale('pA').magnitude\n",
    "                except:\n",
    "                    traces[count_traces]['V'] = sig.rescale('mV').magnitude\n",
    "                count_traces += 1\n",
    "            new_segments.append(traces)\n",
    "        #new_blocks.append(efel_segments)\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "#calculates the dvdt requires path and which sweep to calculate off\n",
    "def dvdt(path, sweep):\n",
    "    table2 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        #check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            for data in traces[sweep]:\n",
    "                times = (data['T']) / 1000\n",
    "                voltages = (data['V'])\n",
    "                times -= times[0]\n",
    "                dt = times[2] - times[1]\n",
    "                detection_level = 0\n",
    "                min_interval = 0.0001\n",
    "                spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                isis = np.diff(spike_times)\n",
    "                time_before = .018\n",
    "                time_after = .015\n",
    "                times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                spike_voltages = []\n",
    "                for i in range(0, len(spike_times)):\n",
    "                    if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                        spike_voltages.append(\n",
    "                            segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                dvdt_threshold = 10\n",
    "                dvdt = find_slopes(mean_spike_voltages, dt)\n",
    "                i = 1\n",
    "                while dvdt[i] < dvdt_threshold:\n",
    "                    i += 1\n",
    "                v0 = mean_spike_voltages[i - 1]\n",
    "                v1 = mean_spike_voltages[i]\n",
    "                dvdt0 = dvdt[i - 1]\n",
    "                dvdt1 = dvdt[i]\n",
    "                v_threshold = v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0)\n",
    "                pandas_dvdt = pd.DataFrame(dvdt)\n",
    "                pandas_dvdt.rename(columns={0: filename}, inplace=True)  #naming the columns!\n",
    "                pandas_membrane_voltages = pd.DataFrame(mean_spike_voltages)\n",
    "            table2.append(pandas_dvdt)\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "\n",
    "            df_concat.to_excel('dvdt' + 'master_file.xlsx', index=False)\n",
    "    return (df_concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this is the workhorse function. It requires the path for a folder that contains perf patch spont firing axograph files\n",
    "#it also requires sweep start and end and time start and time end. These are important because the sweeps are\n",
    "#split half in unperturbed firing and half with zero mean noise injection, which are not important for the current\n",
    "#study\n",
    "#the code returns many dataframes that are used for downstream analysis including plotting and calculation of \n",
    "#mean and minimum voltages\n",
    "#these use cases are exampled below\n",
    "def collect_isis_global_excel(path1, sample_rate, sweep_start, sweep_end, time_start_s, time_end_s, metadata):\n",
    "    os.chdir(path1)\n",
    "    num_points = 1000\n",
    "    sampling_rate = sample_rate\n",
    "    time_start = time_start_s * sampling_rate\n",
    "    time_end = time_end_s * sampling_rate\n",
    "    global_mean_traces = []\n",
    "    global_mean_isi = []\n",
    "    filename_list = []\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "\n",
    "    for filename in file_names:\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            filename_list.append(filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            if len(traces) > 0:\n",
    "                file_results = []\n",
    "                average_isi = []\n",
    "                for trace in traces:\n",
    "                    for p in trace:\n",
    "                        times = (p['T'])\n",
    "                        times = times[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        voltages = (p['V']).flatten()\n",
    "                        voltages = voltages[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        times -= times[0]\n",
    "                        dt = times[2] - times[1]\n",
    "                        detection_level = -10\n",
    "                        min_interval = 0.0001\n",
    "                        spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                        interspike_intervals = np.diff(spike_times)\n",
    "                        if len(interspike_intervals) > 0:\n",
    "                            avg_isi_1 = np.mean(interspike_intervals)\n",
    "                            average_isi.append(avg_isi_1)\n",
    "                            resampled_isi_voltage_arrays = []\n",
    "                            for i in range(len(interspike_intervals)):\n",
    "                                start_time = spike_times[i]\n",
    "                                end_time = spike_times[i] + interspike_intervals[i]\n",
    "                                times_rel = np.linspace(0, 1, num_points)\n",
    "                                isi_voltage_array = segment(voltages, dt, start_time, end_time)\n",
    "                                resampled_isi_voltage_arrays.append(\n",
    "                                    np.interp(times_rel, np.linspace(0, 1, len(isi_voltage_array)), isi_voltage_array))\n",
    "                            mean_resampled_isi_voltages = np.mean(\n",
    "                                np.concatenate(resampled_isi_voltage_arrays).reshape(len(resampled_isi_voltage_arrays),\n",
    "                                                                                     num_points), axis=0)\n",
    "                            file_results.append(mean_resampled_isi_voltages)\n",
    "            isi_file_mean = np.mean(np.array(average_isi))\n",
    "            global_mean_isi.append(isi_file_mean)\n",
    "            file_mean = np.mean(np.array(file_results), axis=0)\n",
    "            global_mean_traces.append(file_mean)\n",
    "    filename_df = pd.DataFrame(filename_list)\n",
    "    filename_df.rename(columns={0: 'filename'}, inplace=True)\n",
    "    global_mean_isis = np.array(global_mean_isi)\n",
    "    global_mean_isi_df = pd.DataFrame(global_mean_isis)\n",
    "    global_mean_isi_df = pd.concat([filename_df, global_mean_isi_df], axis=1)\n",
    "    global_mean_isi_df.set_index('filename', inplace=True)\n",
    "    global_mean_isi_df.sort_index(inplace=True)\n",
    "    global_mean_isis = global_mean_isi_df.values[:, 0]\n",
    "    global_mean_isi_df.to_excel(metadata + '_global_mean_isi.xlsx')\n",
    "    isi_final_mean = np.mean(global_mean_isi, axis=0)\n",
    "    isi_final_mean = np.array([[isi_final_mean]])\n",
    "    isi_final_mean_df = pd.DataFrame(isi_final_mean)\n",
    "    display(isi_final_mean_df)\n",
    "    isi_final_mean_df.to_excel(metadata + '_isi_final_mean.xlsx')\n",
    "\n",
    "    global_mean_traces = np.array(global_mean_traces)\n",
    "    global_mean_traces_df = pd.DataFrame(global_mean_traces)\n",
    "    global_mean_traces_df = pd.concat([filename_df, global_mean_traces_df], axis=1)\n",
    "    global_mean_traces_df.set_index('filename', inplace=True)\n",
    "    global_mean_traces_df.sort_index(inplace=True)\n",
    "    global_mean_traces = np.array(global_mean_traces_df)\n",
    "    global_mean_traces_df.to_excel(metadata + '_global_mean_traces.xlsx')\n",
    "\n",
    "    final_mean_trace = np.mean(global_mean_traces, axis=0)\n",
    "    final_mean_trace_df = pd.DataFrame(final_mean_trace)\n",
    "    final_mean_trace_df.to_excel(metadata + '_final_mean_trace.xlsx')\n",
    "\n",
    "    file_results = np.array(file_results)\n",
    "    file_results_df = pd.DataFrame(file_results)\n",
    "    file_results_df = pd.concat([filename_df, file_results_df], axis=1)\n",
    "    file_results_df.set_index('filename', inplace=True)\n",
    "    file_results_df.sort_index(inplace=True)\n",
    "    file_results_df.to_excel(metadata + '_total_trajectories.xlsx')\n",
    "    print('Analysis complete')\n",
    "    return final_mean_trace, isi_final_mean, global_mean_traces, file_results, global_mean_isis\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "def plot_trajectory_independent(mean_trace, avg_isi, total_trajectory_voltages, color):\n",
    "    dVdt = np.gradient(mean_trace, axis=0)\n",
    "    slope = np.mean(dVdt / 0.1)\n",
    "    avg_isis = np.linspace(0, avg_isi * len(mean_trace), len(mean_trace))\n",
    "    ci = 1.96 * np.std(np.array(total_trajectory_voltages), axis=0) / np.sqrt(len(total_trajectory_voltages))\n",
    "\n",
    "    # calculate duration of each trace\n",
    "    duration = len(mean_trace) * avg_isi\n",
    "\n",
    "    # calculate scaling factor for x-axis\n",
    "    scale = duration / len(mean_trace)\n",
    "\n",
    "    # plot mean trace with actual duration\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(0, duration, scale), mean_trace, color=color, label='Mean Trace')\n",
    "    #ax.fill_between(np.arange(0, duration, scale), mean_trace - ci, mean_trace + ci, alpha=0.3, color='#848482', label='95% CI')\n",
    "    ax.set_xlabel('Time from spike (ms)')\n",
    "    ax.set_ylabel('Membrane Potential (mV)')\n",
    "    ax.set_title('ISI voltage trajectories')\n",
    "\n",
    "    ax.legend(loc='lower right', borderpad=0.1, labelspacing=.3)\n",
    "    plt.ylim(-65, -30)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_voltage_trajectories_two(mean_trace1, mean_trace2, avg_isi1, avg_isi2, total_trajectory_voltages1, total_trajectory_voltages2, mean_trace1_label, mean_trace2_label, mean_trace1_color, mean_trace2_color, save, metadata):\n",
    "\n",
    "    # Calculate first baseline statistics\n",
    "    dVdt1 = np.gradient(mean_trace1, axis=0)\n",
    "    slope1 = np.mean(dVdt1 / 0.1)\n",
    "    avg_isis1 = np.linspace(0, avg_isi1 * len(mean_trace1), len(mean_trace1))\n",
    "    #ci1 = 1.96 * np.std(np.array(total_trajectory_voltages1), axis=0) / np.sqrt(len(total_trajectory_voltages1))\n",
    "\n",
    "    # Calculate second baseline statistics\n",
    "    dVdt2 = np.gradient(mean_trace2, axis=0)\n",
    "    slope2 = np.mean(dVdt2 / 0.1)\n",
    "    avg_isis2 = np.linspace(0, avg_isi2 * len(mean_trace2), len(mean_trace2))\n",
    "    #ci2 = 1.96 * np.std(np.array(total_trajectory_voltages2), axis=0) / np.sqrt(len(total_trajectory_voltages2))\n",
    "\n",
    "    # Calculate duration of each trace\n",
    "    num_points = 1000\n",
    "    duration1 = (len(mean_trace1) * avg_isi1) / num_points-1\n",
    "    duration2 = (len(mean_trace2) * avg_isi2) / num_points-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate scaling factors for x-axis\n",
    "    scale1 = duration1 / len(mean_trace1)\n",
    "    scale2 = duration2 / len(mean_trace2)\n",
    "\n",
    "\n",
    "    # plot mean traces with actual durations\n",
    "    fig, ax = plt.subplots(figsize=(2.75, 2.75))\n",
    "    ax.plot(np.arange(0, duration1, scale1)[:1000], mean_trace1, color=mean_trace1_color, label=mean_trace1_label, linewidth=0.5)\n",
    "    ax.plot(np.arange(0, duration2, scale2)[:1000], mean_trace2, color=mean_trace2_color, label=mean_trace2_label, linewidth=0.5)\n",
    "    # ax.fill_between(np.arange(0, duration1, scale1), mean_trace1 - ci1, mean_trace1 + ci1, alpha=0.3,\n",
    "    #                 color='#848482', label='95% CI (Trace 1)')\n",
    "    # ax.fill_between(np.arange(0, duration2, scale2), mean_trace2 - ci2, mean_trace2 + ci2, alpha=0.3,\n",
    "    #                 color='red', label='95% CI (Trace 2)')\n",
    "    ax.set_xlabel('Time from spike (ms)', fontsize=9)\n",
    "    ax.set_ylabel('Membrane Potential (mV)', fontsize=9)\n",
    "    # ax.set_title('ISI voltage trajectories')\n",
    "    ax.legend(loc='lower right', borderpad=0.4, labelspacing=.1, fontsize=7)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=8, width=0.5)  # You can adjust the font size as needed\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.50)\n",
    "    ax.spines['left'].set_linewidth(0.50)\n",
    "\n",
    "    min_value = min(np.min(mean_trace1), np.min(mean_trace2))\n",
    "    #ax.legend(fontsize=14)\n",
    "    plt.legend(frameon=False, edgecolor='none', fontsize='large', handlelength=2, handleheight=1, loc='lower right')\n",
    "    #set major ticks on the x and y axis\n",
    "\n",
    "    #x.tick_params(axis='x', labelsize=14)\n",
    "    plt.ylim(min_value-1, -30)\n",
    "\n",
    "    if save == True:\n",
    "        os.chdir('/Users/HBLANKEN/Library/CloudStorage/OneDrive-UniversityofOklahoma/Beckstead Lab/DA-AD paper files/Grand collection of axograph files/Noise Traces/Figures')\n",
    "        plt.savefig(metadata + '.pdf', dpi=600, bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#uses output from the collect average ISI to then calculate the slope and mean voltage, this code was superceded by \n",
    "#the 50ms window code, but still was useful for tests\n",
    "def calculate_slope_and_mean_voltage(global_mean_traces, global_mean_isis, save_path, metadata):\n",
    "    os.chdir(save_path)\n",
    "    slopes = []\n",
    "    mean_voltages = []\n",
    "    min_voltages = []\n",
    "    for i, trace in enumerate(global_mean_traces):\n",
    "        if len(trace) == 0:\n",
    "            print(f\"Empty trace found at index {i}\")\n",
    "            continue\n",
    "        isi = global_mean_isis[i]\n",
    "\n",
    "        # Scale the trace based on the actual length of the ISI\n",
    "        time = np.linspace(0, isi, len(trace))\n",
    "\n",
    "        slope_start_voltage_idx = int(0.3 * len(trace))  # find index of minimum voltage\n",
    "        end_idx_slope = int(0.7 * len(trace))\n",
    "        # print('Start Slope:', slope_start_voltage_idx)\n",
    "        # print('End Slope:', end_idx_slope)\n",
    "        # print('Time:', time[slope_start_voltage_idx:end_idx_slope])\n",
    "\n",
    "        min_voltage_index = np.argmin(trace)\n",
    "        mean_voltage_end_idx = int(0.98 * len(trace))\n",
    "\n",
    "        plt.plot(time[slope_start_voltage_idx:end_idx_slope], trace[slope_start_voltage_idx:end_idx_slope])\n",
    "\n",
    "        # Calculate the slope using the scaled trace\n",
    "        slope = np.gradient(trace[slope_start_voltage_idx:end_idx_slope], time[slope_start_voltage_idx:end_idx_slope]) * 1000\n",
    "\n",
    "        slopes.append(slope.mean())\n",
    "        mean_voltages.append(trace[min_voltage_index:mean_voltage_end_idx].mean())\n",
    "        min_voltages.append(trace[:].min())\n",
    "\n",
    "    slope_df = pd.DataFrame({'slope': slopes})\n",
    "    slope_df.to_excel(metadata + '_slopes.xlsx')\n",
    "    mean_voltage_df = pd.DataFrame({'mean_voltage': mean_voltages})\n",
    "    mean_voltage_df.to_excel(metadata + '_mean_voltages.xlsx')\n",
    "    min_voltage_df = pd.DataFrame({'min_voltage': min_voltages})\n",
    "    min_voltage_df.to_excel(metadata + '_min_voltages.xlsx')\n",
    "\n",
    "    results_df = pd.concat([slope_df, mean_voltage_df, min_voltage_df], axis=1)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#helpful function to determine the total mean spike voltages. useful to detect break ins during perf patch recordings. a sharp shift will \n",
    "#towards a depolarization will be present if perf is ruptured\n",
    "def mean_spike_voltages(path, sweep_start, sweep_end, detection_level, metadata):\n",
    "    spike_voltage_list = []\n",
    "    filename_list = []\n",
    "    os.chdir(path)\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "    for filename in file_names:\n",
    "        # check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            filename_list.append(filename)\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            table2 = []\n",
    "            for data in traces[sweep_start:sweep_end]:\n",
    "                for p in data:\n",
    "                    times = (p['T']) / 1000\n",
    "                    voltages = (p['V'])\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    detection_level = detection_level\n",
    "                    min_interval = 0.005\n",
    "                    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                    time_before = .025\n",
    "                    time_after = .015\n",
    "                    times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                    spike_voltages = []\n",
    "                    for i in range(0, len(spike_times)):\n",
    "                        if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                            spike_voltages.append(\n",
    "                                segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                    spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                    if len(spike_voltage_arrays) > 0:\n",
    "                        mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                        table2.append(mean_spike_voltages)\n",
    "            table3 = np.array(table2)\n",
    "            file_mean_spike_voltages = np.mean(table3, axis=0)\n",
    "            spike_voltage_list.append(file_mean_spike_voltages)\n",
    "    filename_list_df = pd.DataFrame(filename_list)\n",
    "    filename_list_df.columns = ['Filename']\n",
    "    global_spike_voltage_list_np = np.array(spike_voltage_list)\n",
    "    global_spike_voltages_df = pd.DataFrame(global_spike_voltage_list_np)\n",
    "    global_spike_voltages_df = pd.concat([filename_list_df, global_spike_voltages_df], axis=1)\n",
    "    global_spike_voltages_df.set_index('Filename', inplace=True)\n",
    "    global_spike_voltages_df.sort_index(inplace=True)\n",
    "    mean_spike_voltages = global_spike_voltages_df.mean(axis=0)\n",
    "    mean_spike_voltages_df = pd.DataFrame(mean_spike_voltages)\n",
    "    mean_spike_voltages_df.columns = ['Voltage (mV) ' + metadata]\n",
    "    #mean_spike_voltages_df.index = times_rel\n",
    "\n",
    "    return global_spike_voltages_df, mean_spike_voltages_df\n",
    "\n",
    "\n",
    "#phase plane plots can be derived off of the dvdt function here that returns a dataframe\n",
    "def calculate_dVdt(df, dt):\n",
    "    # Convert the DataFrame to numeric, replacing non-numeric values with NaN\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    dVdt_df = pd.DataFrame(columns=df.columns)\n",
    "    for column in df.columns:\n",
    "        voltage = df[column].values\n",
    "        dVdt = (np.gradient(voltage, dt)) / 1000\n",
    "        dVdt_df[column] = dVdt\n",
    "    return dVdt_df\n",
    "\n",
    "\n",
    "#another workhorse code block that takes dataput from the collect ISI voltages function and calcs the mean and min and slope\n",
    "#of the ISI\n",
    "def calculate_slope_and_mean_voltage_90ms_window(global_mean_traces, global_mean_isis, save_path, metadata):\n",
    "    os.chdir(save_path)\n",
    "    slopes = []\n",
    "    mean_voltages = []\n",
    "    min_voltages = []\n",
    "    for i, trace in enumerate(global_mean_traces):\n",
    "        if len(trace) == 0:\n",
    "            print(f\"Empty trace found at index {i}\")\n",
    "            continue\n",
    "        isi = global_mean_isis[i]\n",
    "        # print(isi)\n",
    "\n",
    "        # Scale the trace based on the actual length of the ISI\n",
    "        time = np.linspace(0, isi, len(trace))\n",
    "\n",
    "        slope_start_time = 10  # Start time of the window (e.g., 30% of ISI)\n",
    "        slope_end_time = 100  # End time of the window (e.g., 50ms window)\n",
    "\n",
    "        # Find the indices corresponding to the start and end times of the window\n",
    "        slope_start_idx = np.argmin(np.abs(time - slope_start_time))\n",
    "        slope_end_idx = np.argmin(np.abs(time - slope_end_time))\n",
    "        slope_slope_start_voltage_idx = int(0.3 * len(trace))  # find index of minimum voltage\n",
    "        end_idx_slope_slope = int(0.7 * len(trace))\n",
    "        # print('Start Index:', slope_start_idx)\n",
    "        # print('End Index:', slope_end_idx)\n",
    "\n",
    "        min_voltage_index = np.argmin(trace)\n",
    "        # print(min_voltage_index)\n",
    "        # print(slope_end_idx)\n",
    "        mean_voltage_end_idx = int(0.98 * len(trace))\n",
    "\n",
    "        # Calculate the slope using the scaled trace within the window\n",
    "        slope = np.gradient(trace[slope_slope_start_voltage_idx:end_idx_slope_slope],\n",
    "                            time[slope_slope_start_voltage_idx:end_idx_slope_slope]) * 1000\n",
    "\n",
    "        slopes.append(slope.mean())\n",
    "        mean_voltages.append(trace[slope_start_idx:slope_end_idx].mean())\n",
    "        min_voltages.append(trace[:].min())\n",
    "\n",
    "        plt.plot(time, trace)  # Plot the entire trace\n",
    "        plt.axvline(time[slope_start_idx], color='r', linestyle='--')  # Vertical line at start of window\n",
    "        plt.axvline(time[slope_end_idx], color='r', linestyle='--')  # Vertical line at end of window\n",
    "\n",
    "    slope_df = pd.DataFrame({'slope': slopes})\n",
    "    slope_df.to_excel(metadata + '_slopes_mAHP.xlsx')\n",
    "    mean_voltage_df = pd.DataFrame({'mean_voltage': mean_voltages})\n",
    "    mean_voltage_df.to_excel(metadata + '_mean_voltages_mAHP.xlsx')\n",
    "    min_voltage_df = pd.DataFrame({'min_voltage': min_voltages})\n",
    "    min_voltage_df.to_excel(metadata + '_min_voltages_mAHP.xlsx')\n",
    "\n",
    "    results_df = pd.concat([slope_df, mean_voltage_df, min_voltage_df], axis=1)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "#This code calcs the spikewidth at half max and the apparent spike threshold at \n",
    "def calculate_spike_widths_and_thresholds(df, sampling_rate, dvdt_threshold, metadata, save_path):\n",
    "    os.chdir(save_path)\n",
    "    spike_widths = []\n",
    "    filenames = []\n",
    "    threshold = []\n",
    "    dt = 1 / sampling_rate\n",
    "    #the first thing we have to do is set the index to be the filename so that we dont have any strings in the df, but only if its there, so let write an if statement for it\n",
    "    if 'Filename' in df.columns:\n",
    "        df = df.set_index('Filename')\n",
    "    else:\n",
    "        pass\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract voltage trace and time values from the row\n",
    "        voltages = np.array(row.values)\n",
    "        times = np.arange(0, len(voltages)) / sampling_rate\n",
    "\n",
    "        # Find the first derivative of the voltage trace\n",
    "        dvdt = calculate_dVdt(df, dt)\n",
    "\n",
    "        # Find the start of the action potential (AP_begin_indices)\n",
    "        start_indices = np.where(dvdt >= dvdt_threshold)[0]\n",
    "\n",
    "        if len(start_indices) == 0:\n",
    "            continue\n",
    "        AP_begin_index = start_indices[0]\n",
    "        threshold.append(voltages[AP_begin_index])\n",
    "        # Find the peak of the action potential\n",
    "        peak_index = np.argmax(voltages)\n",
    "\n",
    "        # Calculate the half-max voltage value\n",
    "        half_max_voltage = (voltages[peak_index] + voltages[AP_begin_index]) / 2\n",
    "\n",
    "        # Find the indices corresponding to the time at spike start (AP_rise_indices) and time at spike end (AP_fall_indices)\n",
    "        AP_rise_indices = np.where(voltages >= half_max_voltage)[0]\n",
    "        AP_fall_indices = np.where(voltages <= half_max_voltage)[0]\n",
    "        AP_rise_index = AP_rise_indices[0]\n",
    "        AP_fall_index = AP_fall_indices[-1]\n",
    "\n",
    "        # Calculate the action potential duration at half width\n",
    "        spike_width = (times[AP_fall_index] - times[AP_rise_index]) * 100  # Convert to ms\n",
    "        spike_widths.append(spike_width)\n",
    "\n",
    "        # Extract the filename from the index\n",
    "        filenames.append(index)\n",
    "\n",
    "    spike_width_df = pd.DataFrame({'spike_width_half_max': spike_widths}, index=filenames)\n",
    "    threshold_df = pd.DataFrame({'threshold': threshold}, index=filenames)\n",
    "    final_df = spike_width_df.join(threshold_df)\n",
    "    final_df.to_excel(metadata + '_spike_widths_and_thresholds.xlsx')\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def collect_frequency_and_cv(path1, sample_rate, sweep_start, sweep_end, time_start_s, time_end_s, metadata):\n",
    "    def one_by_isi(x):\n",
    "        return 1 / (x / 1000)\n",
    "\n",
    "    def calc_cv(interspike_intervals):\n",
    "        return np.std(interspike_intervals) / np.mean(interspike_intervals)\n",
    "\n",
    "    os.chdir(path1)\n",
    "    sampling_rate = sample_rate\n",
    "    time_start = time_start_s * sampling_rate\n",
    "    time_end = time_end_s * sampling_rate\n",
    "    global_mean_isi = []\n",
    "    isi_file_means = []\n",
    "    freq_file_means = []\n",
    "    global_mean_cv = []\n",
    "    filename_list = []\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "    for filename in file_names:\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            filename_list.append(filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            if len(traces) > 0:\n",
    "                isi_list = []\n",
    "                cv_list = []\n",
    "                freq_list = []\n",
    "                for trace in traces:\n",
    "                    for p in trace:\n",
    "                        times = (p['T'])\n",
    "                        times = times[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        voltages = (p['V']).flatten()\n",
    "                        voltages = voltages[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        times -= times[0]\n",
    "                        dt = times[2] - times[1]\n",
    "                        detection_level = -10\n",
    "                        min_interval = 0.0001\n",
    "                        spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                        interspike_intervals = np.diff(spike_times)\n",
    "                        if len(interspike_intervals) > 0:\n",
    "                            avg_isi_1 = np.mean(interspike_intervals)\n",
    "                            avg_freq_1 = one_by_isi(avg_isi_1)\n",
    "                            cv_1 = calc_cv(interspike_intervals)\n",
    "                            isi_list.append(avg_isi_1)\n",
    "                            cv_list.append(cv_1)\n",
    "                            freq_list.append(avg_freq_1)\n",
    "                if len(isi_list) > 0:\n",
    "                    freq_means = np.mean(np.array(freq_list))\n",
    "                    freq_file_means.append(freq_means)\n",
    "                    isi_file_mean = np.mean(np.array(isi_list))\n",
    "                    isi_file_means.append(isi_file_mean)\n",
    "                    cv_file_mean = np.mean(np.array(cv_list))\n",
    "                    global_mean_isi.append(isi_file_mean)\n",
    "                    global_mean_cv.append(cv_file_mean)\n",
    "\n",
    "    filename_df = pd.DataFrame(filename_list)\n",
    "    filename_df.rename(columns={0: 'filename'}, inplace=True)\n",
    "    global_mean_isis = np.array(global_mean_isi)\n",
    "    global_mean_isi_df = pd.DataFrame(global_mean_isis)\n",
    "\n",
    "    freq_means_df = pd.DataFrame(freq_file_means)\n",
    "    freq_means_df.rename(columns={0: 'Frequency'}, inplace=True)\n",
    "    freq_means_df = pd.concat([filename_df, freq_means_df], axis=1)\n",
    "    freq_means_df.set_index('filename', inplace=True)\n",
    "    freq_means_df.sort_index(inplace=True)\n",
    "\n",
    "    global_mean_frequencies = global_mean_isi_df.applymap(one_by_isi)\n",
    "    global_mean_frequencies.rename(columns={0: 'Frequency'}, inplace=True)\n",
    "    global_mean_frequencies = pd.concat([filename_df, global_mean_frequencies], axis=1)\n",
    "    global_mean_frequencies.set_index('filename', inplace=True)\n",
    "    global_mean_frequencies.sort_index(inplace=True)\n",
    "\n",
    "    global_mean_cvs = np.array(global_mean_cv)\n",
    "    global_mean_cv_df = pd.DataFrame(global_mean_cvs)\n",
    "    global_mean_cv_df.rename(columns={0: 'CV_ISI'}, inplace=True)\n",
    "    global_mean_cv_df.set_index(global_mean_frequencies.index, inplace=True)\n",
    "\n",
    "    result_df = pd.concat([freq_means_df, global_mean_cv_df], axis=1)\n",
    "    result_df['Metadata'] = metadata\n",
    "#     result_df.to_excel(metadata + '_frequency_and_cv.xlsx')\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a directory, all code for these analyses function for batch processing, and will work on a file's worth of data. They will specifically pull files in the axograph file format, and analyze them for each function. certain functions are dependent on other functions. \n",
    "#the purpose of this example notebook is for a general workflow for analyzing data. We have included a few functions and example data to show how to use the functions.\n",
    "example_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9772a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we will collect all interspike intervals and global mean traces for a set of files. This function will return a dataframe with the interspike intervals and global mean traces for each file, we will call these 'Control'. You can check the params for each function to see the input parameter requirements. \n",
    "control_noise_mean_trace, control_noise_isi, control_noise_global_mean_traces, control_noise_total_trajectories_baseline, control_noise_global_mean_isis = collect_isis_global_excel(example_dir, 20000, 0, 50, 0, 10, 'control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_voltage_trajectory(mean_trace1, avg_isi1, mean_trace1_label, mean_trace1_color, save, metadata):\n",
    "\n",
    "    # Calculate first baseline statistics\n",
    "    dVdt1 = np.gradient(mean_trace1, axis=0)\n",
    "    slope1 = np.mean(dVdt1 / 0.1)\n",
    "    avg_isis1 = np.linspace(0, avg_isi1 * len(mean_trace1), len(mean_trace1))\n",
    "    #ci1 = 1.96 * np.std(np.array(total_trajectory_voltages1), axis=0) / np.sqrt(len(total_trajectory_voltages1))\n",
    "\n",
    "    # Calculate second baseline statistics\n",
    "   \n",
    "    #ci2 = 1.96 * np.std(np.array(total_trajectory_voltages2), axis=0) / np.sqrt(len(total_trajectory_voltages2))\n",
    "\n",
    "    # Calculate duration of each trace\n",
    "    num_points = 1000\n",
    "    duration1 = (len(mean_trace1) * avg_isi1) / num_points-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate scaling factors for x-axis\n",
    "    scale1 = duration1 / len(mean_trace1)\n",
    "\n",
    "\n",
    "\n",
    "    # plot mean traces with actual durations\n",
    "    fig, ax = plt.subplots(figsize=(2.75, 2.75))\n",
    "    ax.plot(np.arange(0, duration1, scale1)[:1000], mean_trace1, color=mean_trace1_color, label=mean_trace1_label, linewidth=0.5)\n",
    "    # ax.fill_between(np.arange(0, duration1, scale1), mean_trace1 - ci1, mean_trace1 + ci1, alpha=0.3,\n",
    "    #                 color='#848482', label='95% CI (Trace 1)')\n",
    "    # ax.fill_between(np.arange(0, duration2, scale2), mean_trace2 - ci2, mean_trace2 + ci2, alpha=0.3,\n",
    "    #                 color='red', label='95% CI (Trace 2)')\n",
    "    ax.set_xlabel('Time from spike (ms)', fontsize=9)\n",
    "    ax.set_ylabel('Membrane Potential (mV)', fontsize=9)\n",
    "    # ax.set_title('ISI voltage trajectories')\n",
    "    ax.legend(loc='lower right', borderpad=0.4, labelspacing=.1, fontsize=7)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=8, width=0.5)  # You can adjust the font size as needed\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.50)\n",
    "    ax.spines['left'].set_linewidth(0.50)\n",
    "\n",
    "    min_value = np.min(mean_trace1)\n",
    "    #ax.legend(fontsize=14)\n",
    "    plt.legend(frameon=False, edgecolor='none', fontsize='large', handlelength=2, handleheight=1, loc='lower right')\n",
    "    #set major ticks on the x and y axis\n",
    "\n",
    "    #x.tick_params(axis='x', labelsize=14)\n",
    "    plt.ylim(min_value-1, -30)\n",
    "\n",
    "    if save == True:\n",
    "        os.chdir('/Users/HBLANKEN/Library/CloudStorage/OneDrive-UniversityofOklahoma/Beckstead Lab/DA-AD paper files/Grand collection of axograph files/Noise Traces/Figures')\n",
    "        plt.savefig(metadata + '.pdf', dpi=600, bbox_inches='tight', transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will plot the voltage trajectories for the control group. This function will plot the voltage trajectories for the control group. You can check the params for each function to see the input parameter requirements.\n",
    "plot_average_voltage_trajectory(control_noise_mean_trace, control_noise_isi, 'Control', 'black', False, 'Control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf00c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we calculate the slopes and mean voltages for the group. This function will calculate the slopes and mean voltages for the group. You can check the params for each function to see the input parameter requirements.\n",
    "#control slopes and mean voltages\n",
    "\n",
    "save_path = 'Output path'\n",
    "control_slopes = calculate_slope_and_mean_voltage_50ms_window(control_noise_global_mean_traces, control_noise_global_mean_isis, save_path, 'control')\n",
    "\n",
    "control_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we collect the spike voltages to work off of\n",
    "control_global_spike_voltages, control_mean_spike_voltages = mean_spike_voltages(os.getcwd(), 0, 50, -10, 'Control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec392a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcualte the spike width and thresholds from the spike voltages\n",
    "calculate_spike_widths_and_thresholds(control_global_spike_voltages, 20_000, 10, 'WT', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we calculate the frequency and cv for the group. This function will calculate the frequency and cv for the group. You can check the params for each function to see the input parameter requirements.\n",
    "collect_frequency_and_cv(os.getcwd(), 20000, 0, -10, 0, 10, 'control')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

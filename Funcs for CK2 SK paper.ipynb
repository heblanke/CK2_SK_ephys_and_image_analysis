{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:04:32.272579Z",
     "start_time": "2024-01-11T19:04:32.270964Z"
    }
   },
   "outputs": [],
   "source": [
    "#Functions for the CK2 SK paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import efel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import matplotlib\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\")\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "#plt.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ee43509bc86bf1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# define some functions\n",
    "# def find_spike_times(voltages, dt, detection_level, min_interval):\n",
    "#     spike_times = []\n",
    "#     last_spike_time = -min_interval\n",
    "#\n",
    "#     for i in range(1, len(voltages)):\n",
    "#         t = i * dt\n",
    "#         if (voltages[i - 1] < detection_level <= voltages[i]) and (t - last_spike_time >= min_interval):\n",
    "#             spike_times.append(t)\n",
    "#             last_spike_time = t\n",
    "#     return spike_times\n",
    "\n",
    "def find_spike_times(voltages, dt, detection_level, min_interval):\n",
    "    spike_times = []\n",
    "    min_amplitude = -23\n",
    "    last_spike_time = -min_interval\n",
    "    for i in range(1, len(voltages)):\n",
    "        t = i * dt\n",
    "        if (voltages[i - 1] < detection_level <= voltages[i]) and (voltages[i] - voltages[i - 1] >= min_amplitude) and (t - last_spike_time >= min_interval):\n",
    "            spike_times.append(t)\n",
    "            last_spike_time = t\n",
    "    return spike_times\n",
    "\n",
    "\n",
    "def group_cvs(values, group_size):\n",
    "    cvs = []\n",
    "    if len(values) >= group_size:\n",
    "        for i in range(0, len(values) - group_size, group_size):\n",
    "            mu = np.mean(values[i:i + group_size])\n",
    "            sigma = np.std(values[i:i + group_size])\n",
    "            cvs.append(sigma / mu)\n",
    "    return cvs\n",
    "\n",
    "\n",
    "def segment(values, dx, x_min, x_max):\n",
    "    return values[round(x_min / dx):round(x_max / dx)]\n",
    "\n",
    "\n",
    "def find_slopes(values, dx):\n",
    "    diffs = np.diff(values)\n",
    "    slopes = [0] * len(values)\n",
    "    slopes[0] = diffs[0] / dx\n",
    "    slopes[-1] = diffs[-1] / dx\n",
    "    for i in range(1, len(values) - 1):\n",
    "        slopes[i] = (diffs[i - 1] + diffs[i]) / (2 * dx)\n",
    "    return (slopes)\n",
    "\n",
    "\n",
    "#use neo to import either voltage or current clamp data in the correct, scaled units!\n",
    "def load_neo_file(file_name, **kwargs):\n",
    "    import neo\n",
    "    reader = neo.io.get_io(file_name)\n",
    "    blocks = reader.read(**kwargs)\n",
    "    new_blocks = []\n",
    "    for bl in blocks:\n",
    "        new_segments = []\n",
    "        for seg in bl.segments:\n",
    "            traces = []\n",
    "            count_traces = 0\n",
    "            analogsignals = seg.analogsignals\n",
    "\n",
    "            for sig in analogsignals:\n",
    "                traces.append({})\n",
    "                traces[count_traces]['T'] = sig.times.rescale('ms').magnitude\n",
    "                #need to write an if statement here for conversion\n",
    "                try:\n",
    "                    traces[count_traces]['A'] = sig.rescale('pA').magnitude\n",
    "                except:\n",
    "                    traces[count_traces]['V'] = sig.rescale('mV').magnitude\n",
    "                count_traces += 1\n",
    "            new_segments.append(traces)\n",
    "        #new_blocks.append(efel_segments)\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def dvdt(path, sweep):\n",
    "    table2 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        #check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            for data in traces[sweep]:\n",
    "                times = (data['T']) / 1000\n",
    "                voltages = (data['V'])\n",
    "                times -= times[0]\n",
    "                dt = times[2] - times[1]\n",
    "                detection_level = 0\n",
    "                min_interval = 0.0001\n",
    "                spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                isis = np.diff(spike_times)\n",
    "                time_before = .018\n",
    "                time_after = .015\n",
    "                times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                spike_voltages = []\n",
    "                for i in range(0, len(spike_times)):\n",
    "                    if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                        spike_voltages.append(\n",
    "                            segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                dvdt_threshold = 20\n",
    "                dvdt = find_slopes(mean_spike_voltages, dt)\n",
    "                i = 1\n",
    "                while dvdt[i] < dvdt_threshold:\n",
    "                    i += 1\n",
    "                v0 = mean_spike_voltages[i - 1]\n",
    "                v1 = mean_spike_voltages[i]\n",
    "                dvdt0 = dvdt[i - 1]\n",
    "                dvdt1 = dvdt[i]\n",
    "                v_threshold = v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0)\n",
    "                pandas_dvdt = pd.DataFrame(dvdt)\n",
    "                pandas_dvdt.rename(columns={0: filename}, inplace=True)  #naming the columns!\n",
    "                pandas_membrane_voltages = pd.DataFrame(mean_spike_voltages)\n",
    "            table2.append(pandas_dvdt)\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "\n",
    "            df_concat.to_excel('dvdt' + 'master_file.xlsx', index=False)\n",
    "    return (df_concat)\n",
    "\n",
    "\n",
    "#write some functions for the rest of this stuff\n",
    "def analyze_feature(path, feature):\n",
    "    table2 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        table = pd.DataFrame(columns=[feature])  #create a table that has columns with the name you want\n",
    "        table.name = feature  #the tables name\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):  #check for the filetype\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500,\n",
    "                                             stim_end=1500)  #load the trace, and define stim start and stop\n",
    "            for data in traces:  #loop through these guys\n",
    "                #table.rename(columns={feature:filename}, inplace=True) #renaming the columns with the correct file !\n",
    "                feature_values = efel.getFeatureValues(data, [feature], raise_warnings=None)[\n",
    "                    0]  #this is the feature extraction\n",
    "                if feature_values[feature] is not None:\n",
    "                    # Define the parameters for detection\n",
    "                    efel.api.setThreshold(-10)  # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20)  # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature][0]\n",
    "\n",
    "                else:\n",
    "                    efel.api.setThreshold(-10)  # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20)  # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature]\n",
    "\n",
    "            table2.append(table)\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "            table.rename(columns={feature: filename}, inplace=True)  #renaming the columns with the correct file !\n",
    "            #block of code to combine all of the generated excel workbooks into a single workbook\n",
    "            df_concat.to_excel(feature + 'master_file.xlsx', index=False)\n",
    "    Current_injected = np.linspace(-100.0, 500.0, num=61)\n",
    "    table2 = df_concat.assign(Current_injected=Current_injected)\n",
    "    #lineplot = df_concat.plot()\n",
    "    #sns_lineplot = sns.relplot(data = table2, x = \"Spikecount_stimint\", y = 'Current_injected', kind=\"line\")\n",
    "    return (df_concat)\n",
    "\n",
    "def mean_spike_voltages(path, sweep_start, sweep_end, metadata):\n",
    "    spike_voltage_list = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        #check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            table2 = []\n",
    "            for data in traces[sweep_start:sweep_end]:\n",
    "                for p in data:\n",
    "                    times = (p['T']) / 1000\n",
    "                    voltages = (p['V'])\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    detection_level = -30\n",
    "                    min_interval = 0.0001\n",
    "                    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                    time_before = .018\n",
    "                    time_after = .015\n",
    "                    times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                    spike_voltages = []\n",
    "                    for i in range(0, len(spike_times)):\n",
    "                        if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                            spike_voltages.append(\n",
    "                                segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                    spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                    mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                    table2.append(mean_spike_voltages)\n",
    "            table3 = np.array(table2)\n",
    "            spike_voltage_list.append(table3)\n",
    "            table3_df = pd.DataFrame(table3)\n",
    "    global_mean_spike_voltages = pd.concat([pd.DataFrame(x) for x in spike_voltage_list], axis=0).mean(axis=0)\n",
    "    global_mean_spike_voltages_df = pd.DataFrame(global_mean_spike_voltages)\n",
    "    global_mean_spike_voltages_df.columns = ['Voltage (mV) ' + metadata]\n",
    "    global_mean_spike_voltages_df.index = times_rel\n",
    "\n",
    "    return global_mean_spike_voltages_df\n",
    "\n",
    "\n",
    "def A_current(path, genotype):\n",
    "    os.chdir(path)\n",
    "    table2 = []\n",
    "    table3 = []\n",
    "    table4 = []\n",
    "    for file_name in os.listdir():\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            file = load_neo_file(file_name)\n",
    "\n",
    "            def monoExp(x, m, t, b):\n",
    "                return m * np.exp(-t * x) + b\n",
    "\n",
    "            for traces in file:\n",
    "                for data in traces:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][10100:17500]\n",
    "                    amps1 = data['A'][10100:17500]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][2800:3000])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    max_amp1 = np.amax(amps1)  #find the maximum of the baseline subtracted amplitudes\n",
    "                    #working on integration\n",
    "                    flat_amps_1 = np.ndarray.flatten(\n",
    "                        amps1)  #here we have to flatten our array into a single dimension so we can integrate\n",
    "                    integrate1 = np.trapz(flat_amps_1)  #lets integrate this function using the numpy trapezius function\n",
    "                    #this is for the substraction of the next part, so that we can get the substracted information\n",
    "                    times2 = data['T'][\n",
    "                             35100:42500]  #in order to extract a-current, we need to subtract from the inactivated standing current\n",
    "                    amps2 = data['A'][\n",
    "                            35100:42500]  #in order to extract a-current, we need to subtract from the inactivated standing current\n",
    "                    #plt.plot(times2, amps2) #inspect\n",
    "                    amps2 = amps2 - baseline  #find the baseline subtracted values for the inactivated standing current\n",
    "                    max_amp2 = np.amax(amps2)  #find the max amplitude of that current\n",
    "                    flat_amps_2 = np.ndarray.flatten(\n",
    "                        amps2)  #flatten the baseline substracted inactivated portion for integration\n",
    "                    integrate2 = np.trapz(flat_amps_2)  #integrate flattened baseline substracted sweeps\n",
    "                    #now we need to substract that information to get the A current values\n",
    "                    a_current_max_amp = max_amp1 - max_amp2  #this and the next calls are for the actual data we want, here max amp\n",
    "                    a_current_auc = integrate1 - integrate2  #and here AUC\n",
    "                    #from this point foward we are trying to put data together for iteration\n",
    "                    #print('A-Current Max Amp is ', a_current_max_amp, 'pA')\n",
    "                    #print('A-Current AUC is', a_current_auc, 'pA*s')\n",
    "                    #generate a dataframe, must pass a 2d array\n",
    "                    a_current_max_amp_array = np.array(a_current_max_amp, ndmin=2)\n",
    "                    max_amp_df = pd.DataFrame(a_current_max_amp_array)\n",
    "                    max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "                    max_amp_df['Genotype'] = genotype\n",
    "\n",
    "                    #generate a dataframe, must mass a 2d array\n",
    "                    a_current_auc_array = np.array(a_current_auc, ndmin=2)\n",
    "                    auc_df = pd.DataFrame(a_current_auc_array)\n",
    "                    auc_df['file_name'] = file_name  #adding a column to add the filename\n",
    "\n",
    "                    #fit a curve using scipy functionality - these params seem to work for a-current\n",
    "                    p0 = [500, .001, 50]  #values near what we expect   #here\n",
    "                    params, cv = scipy.optimize.curve_fit(monoExp, times1, flat_amps_1, p0, bounds=(-np.inf, np.inf),\n",
    "                                                          maxfev=50000)  #here\n",
    "                    m, t, b = params  #here\n",
    "                    #m, t = params\n",
    "                    sampleRate = 10_000  #hz\n",
    "                    tauSec = (1 / t) / sampleRate\n",
    "                    #determine quality of fit\n",
    "                    squaredDiffs = np.square(flat_amps_1 - monoExp(times1, m, t, b))  #here\n",
    "                    squaredDiffsFromMean = np.square(flat_amps_1 - np.mean(flat_amps_1))\n",
    "                    rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                        squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "                    #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "                    #plot results\n",
    "                    #plt.plot(times1, flat_amps_1, '.', label=\"data\")\n",
    "                    #plt.plot(times1, monoExp(times1, m, t, b), '--', label=\"fitted\")  #here\n",
    "                    #plt.show()\n",
    "                    #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "                    #inspect the params\n",
    "                    #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "                    #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "                    tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "\n",
    "                    tau_df = pd.DataFrame(tau_array)\n",
    "                    tau_df['file_name'] = file_name  #adding a column to add the filename\n",
    "                table2.append(max_amp_df)\n",
    "\n",
    "                table3.append(auc_df)\n",
    "                table4.append(tau_df)\n",
    "\n",
    "        amp_concat = pd.concat(table2, ignore_index=True, axis=0)\n",
    "        amp_concat.rename(columns={0: 'Max Amplitude(pA)'}, inplace=True)\n",
    "        #amp_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "        every_4th_sweep_amp = amp_concat[amp_concat.index % 5 == 3]\n",
    "        every_4th_sweep_amp.to_excel('a_current_amp' + '.xlsx', index=False)\n",
    "\n",
    "        auc_concat = pd.concat(table3, ignore_index=True, axis=0)\n",
    "        auc_concat.rename(columns={0: 'AUC (pA*s)'}, inplace=True)\n",
    "        #auc_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "        every_4th_sweep_auc = auc_concat[amp_concat.index % 5 == 3]\n",
    "        every_4th_sweep_auc.to_excel('a_current_auc' + '.xlsx', index=False)\n",
    "\n",
    "        tau_concat = pd.concat(table4, ignore_index=True, axis=0)\n",
    "        tau_concat.rename(columns={0: 'Decay Tau (ms)'}, inplace=True)\n",
    "        #tau_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "        every_4th_sweep_tau = tau_concat[amp_concat.index % 5 == 3]\n",
    "        every_4th_sweep_tau.to_excel('a_current_tau' + '.xlsx', index=False)\n",
    "    return display(every_4th_sweep_amp), display(every_4th_sweep_auc), display(every_4th_sweep_tau)\n",
    "\n",
    "\n",
    "def standing_K_current(path, genotype):\n",
    "    os.chdir(path)\n",
    "    table2 = []\n",
    "    table3 = []\n",
    "    table4 = []\n",
    "    for file_name in os.listdir():\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            file = load_neo_file(file_name)\n",
    "            for traces in file:\n",
    "                for data in traces:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][10100:17500]\n",
    "                    amps1 = data['A'][10100:17500]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][2800:3000])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    max_amp1 = np.amax(amps1)  #find the maximum of the baseline subtracted amplitudes\n",
    "                    #working on integration\n",
    "                    flat_amps_1 = np.ndarray.flatten(\n",
    "                        amps1)  #here we have to flatten our array into a single dimension so we can integrate\n",
    "                    integrate1 = np.trapz(flat_amps_1)  #lets integrate this function using the numpy trapezius function\n",
    "                    #this is for the substraction of the next part, so that we can get the substracted information\n",
    "                    times2 = data['T'][\n",
    "                             41444:45000]  #in order to extract a-current, we need to subtract from the inactivated standing current\n",
    "                    amps2 = data['A'][\n",
    "                            41444:45000]  #in order to extract a-current, we need to subtract from the inactivated standing current\n",
    "                    #plt.plot(times2, amps2) #inspect\n",
    "                    amps2 = amps2 - baseline  #find the baseline subtracted values for the inactivated standing current\n",
    "                    max_amp2 = np.amax(amps2)  #find the max amplitude of that current\n",
    "                    flat_amps_2 = np.ndarray.flatten(\n",
    "                        amps2)  #flatten the baseline substracted inactivated portion for integration\n",
    "                    integrate2 = np.trapz(flat_amps_2)  #integrate flattened baseline substracted sweeps\n",
    "                    #now we need to substract that information to get the A current values\n",
    "                    standing_k_max_amp = max_amp2  #this and the next calls are for the actual data we want, here max amp\n",
    "                    standing_k_auc = integrate2  #and here AUC\n",
    "                    #from this point foward we are trying to put data together for iteration\n",
    "                    #print('A-Current Max Amp is ', a_current_max_amp, 'pA')\n",
    "                    #print('A-Current AUC is', a_current_auc, 'pA*s')\n",
    "                    #generate a dataframe, must pass a 2d array\n",
    "                    a_current_max_amp_array = np.array(standing_k_max_amp, ndmin=2)\n",
    "                    max_amp_df = pd.DataFrame(a_current_max_amp_array)\n",
    "                    max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "                    max_amp_df['Genotype'] = genotype\n",
    "\n",
    "                    #generate a dataframe, must mass a 2d array\n",
    "                    a_current_auc_array = np.array(standing_k_auc, ndmin=2)\n",
    "                    auc_df = pd.DataFrame(a_current_auc_array)\n",
    "                    auc_df['file_name'] = file_name  #adding a column to add the filename\n",
    "\n",
    "                    #fit a curve using scipy functionality - these params seem to work for a-current\n",
    "\n",
    "                table2.append(max_amp_df)\n",
    "                table3.append(auc_df)\n",
    "\n",
    "            amp_concat = pd.concat(table2, ignore_index=True, axis=0)\n",
    "            amp_concat.rename(columns={0: 'Max Amplitude(pA)'}, inplace=True)\n",
    "            #amp_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "            every_4th_sweep_amp = amp_concat[amp_concat.index % 5 == 3]\n",
    "            every_4th_sweep_amp.to_excel('Standing K Amp' + '.xlsx', index=False)\n",
    "\n",
    "            auc_concat = pd.concat(table3, ignore_index=True, axis=0)\n",
    "            auc_concat.rename(columns={0: 'AUC (pA*s)'}, inplace=True)\n",
    "            #auc_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "            every_4th_sweep_auc = auc_concat[amp_concat.index % 5 == 3]\n",
    "            every_4th_sweep_auc.to_excel('Standing K AUC' + '.xlsx', index=False)\n",
    "    return display(every_4th_sweep_amp), display(every_4th_sweep_auc)\n",
    "\n",
    "\n",
    "def ahc_analysis(path, genotype):\n",
    "    def monoExp(x, m, t, b):\n",
    "        return m * np.exp(-t * x) + b\n",
    "\n",
    "    ahc_append = []\n",
    "    ahc_auc_append = []\n",
    "    ahc_max_amp_append = []\n",
    "    auc_append = []\n",
    "    tau_append = []\n",
    "    ahc_max_amp_df_append = []\n",
    "    os.chdir(path)\n",
    "    for file_name in os.listdir():\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][322002:328000]  #this is 16.101 to 16.213\n",
    "                    amps1 = data['A'][322002:328000]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][318400:319600])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    amps1_df = pd.DataFrame(amps1)  #we generated all the amps into a dataframe, check\n",
    "                    flat_times = np.ndarray.flatten(\n",
    "                        times1)  #we have all of the times into a flattened numpy array, check\n",
    "                    tabla.append(amps1_df)  #and we appended all of the amps into a dataframe\n",
    "                amps_concat = pd.concat(tabla,\n",
    "                                        axis=1)  #this is correct - we put all the amps for a given trace into a dataframe\n",
    "            #now we need to average those dataframes by row\n",
    "            averaged_sk_trace = amps_concat.mean(\n",
    "                axis=1)  #woohooo this is the averaged SK trace in a pd.DF, this is what we want to work with from here on out!\n",
    "\n",
    "            #Block of code of AHC Max Amp !\n",
    "            ahc_max_amp = pd.DataFrame.max(\n",
    "                averaged_sk_trace)  #max of the whole ahc, not just sk component, This is the value!!\n",
    "            #figure out to how put this in a format that can be read and concatenated\n",
    "            ahc_max_amp_array = np.array(ahc_max_amp, ndmin=2)  #these lines are new\n",
    "            ahc_max_amp_df = pd.DataFrame(\n",
    "                ahc_max_amp_array)  #these lines are new - this line of code produces the correct values but does not add them into an appended dataframe. which is obviously a problem. the task is to get these and all the values from the other two measures into a single dataframe so that in the future we may compare across groups within python<3\n",
    "            ahc_max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "            ahc_max_amp_df['Genotype'] = genotype\n",
    "            ahc_append.append(ahc_max_amp_df)  #Here!\n",
    "            ahc_max_amp_concat_df = pd.concat(ahc_append)  #This is the variable for ahc\n",
    "\n",
    "            #Block of code for AHC AUC\n",
    "            averaged_sk_trace_as_np = averaged_sk_trace.to_numpy()\n",
    "            flattened_average_sk_trace = np.ndarray.flatten(averaged_sk_trace_as_np)\n",
    "            ahc_auc = np.trapz(flattened_average_sk_trace)  #this is the auc of the whole ahc\n",
    "            #bit of code to get the ahc auc into readable condition\n",
    "            ahc_auc_array = np.array(ahc_auc, ndmin=2)\n",
    "            ahc_auc_df = pd.DataFrame(ahc_auc_array)  #here we have the auc data in a dataframe\n",
    "            ahc_auc_df['file_name'] = file_name\n",
    "            ahc_auc_append.append(ahc_auc_df)\n",
    "            ahc_auc_concat_df = pd.concat(ahc_auc_append)  #this is the variable for auc\n",
    "\n",
    "            #Block of code for kinetics\n",
    "            trace_for_kinetics = flattened_average_sk_trace[30:]\n",
    "            times_for_kinetics = flat_times[30:]\n",
    "            trace_for_kinetics_pd = pd.DataFrame(trace_for_kinetics)\n",
    "            #trace_for_kinetics_pd.to_excel(\"trace_for_kinetics_pd.xlsx\")\n",
    "            times_for_kinetics_pd = pd.DataFrame(times_for_kinetics)\n",
    "            #times_for_kinetics_pd.to_excel(\"times_for_kinetics_pd.xlsx\")\n",
    "\n",
    "            #fit the curve for inactivation tau\n",
    "            p0 = [1.187 * 10 ** 150., .02, +16]  #values near what we expect   #here\n",
    "            params, cv = scipy.optimize.curve_fit(monoExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "                                                  bounds=(-np.inf, np.inf),\n",
    "                                                  maxfev=100000)  #here  #this fits the training curve with an r-squared of 0.97\n",
    "            m, t, b = params  #here\n",
    "            #m, t = params\n",
    "            sampleRate = 20_000  #hz\n",
    "            tauSec = (1 / t) / sampleRate\n",
    "\n",
    "            #determine quality of fit\n",
    "            squaredDiffs = np.square(trace_for_kinetics - monoExp(times_for_kinetics, m, t, b))  #here\n",
    "            squaredDiffsFromMean = np.square(trace_for_kinetics - np.mean(trace_for_kinetics))\n",
    "            rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "            #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "            #plot results\n",
    "            #plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "            #plt.plot(times_for_kinetics, monoExp(times_for_kinetics, m, t, b), '--', label=\"fitted\")  #here\n",
    "            plt.show()\n",
    "            #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "            #inspect the params\n",
    "            #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "            #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "            plt.show()\n",
    "            tau_flat_ms = tauSec * 1e4\n",
    "\n",
    "            #Bit of code to get tau into working order\n",
    "            tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "            tau_df = pd.DataFrame(tau_array)\n",
    "            tau_df['file_name'] = file_name\n",
    "            tau_append.append(tau_df)\n",
    "            tau_concat_df = pd.concat(tau_append)  #this is the variable for tau\n",
    "\n",
    "    #lets rename columns and export to excel for each of our metrics\n",
    "    ahc_max_amp_concat_df.rename(columns={0: 'AHC Max Amplitude (pA)'}, inplace=True)\n",
    "    ahc_max_amp_concat_df.to_excel('ahc_max_amp_' + '.xlsx', index=False)\n",
    "\n",
    "    ahc_auc_concat_df.rename(columns={0: 'AHC AUC (pA*s)'}, inplace=True)\n",
    "    ahc_auc_concat_df.to_excel('ahc_auc' + '.xlsx', index=False)\n",
    "\n",
    "    tau_concat_df.rename(columns={0: 'Decay Tau (ms)'}, inplace=True)\n",
    "    tau_concat_df.to_excel('ahc_tau' + '.xlsx', index=False)\n",
    "\n",
    "    return display(ahc_max_amp_concat_df), display(ahc_auc_concat_df), display(tau_concat_df)\n",
    "\n",
    "\n",
    "def sk_analysis(path, genotype):\n",
    "    def monoExp(x, m, t, b):\n",
    "        return m * np.exp(-t * x) + b\n",
    "\n",
    "    ahc_append = []\n",
    "    ahc_auc_append = []\n",
    "\n",
    "    ahc_max_amp_append = []\n",
    "    auc_append = []\n",
    "    tau_append = []\n",
    "    ahc_max_amp_df_append = []\n",
    "    os.chdir(path)\n",
    "\n",
    "    for file_name in os.listdir():\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][322440:328000]  #this is 16.122 to 16.213\n",
    "                    amps1 = data['A'][322440:328000]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][318400:319600])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    amps1_df = pd.DataFrame(amps1)  #we generated all the amps into a dataframe, check\n",
    "                    flat_times = np.ndarray.flatten(\n",
    "                        times1)  #we have all of the times into a flattened numpy array, check\n",
    "                    tabla.append(amps1_df)  #and we appended all of the amps into a dataframe\n",
    "                amps_concat = pd.concat(tabla,\n",
    "                                        axis=1)  #this is correct - we put all the amps for a given trace into a dataframe\n",
    "            #now we need to average those dataframes by row\n",
    "            averaged_sk_trace = amps_concat.mean(\n",
    "                axis=1)  #woohooo this is the averaged SK trace in a pd.DF, this is what we want to work with from here on out!\n",
    "\n",
    "            #Block of code of AHC Max Amp !\n",
    "            ahc_max_amp = pd.DataFrame.max(\n",
    "                averaged_sk_trace)  #max of the whole ahc, not just sk component, This is the value!!\n",
    "            #figure out to how put this in a format that can be read and concatenated\n",
    "            ahc_max_amp_array = np.array(ahc_max_amp, ndmin=2)  #these lines are new\n",
    "            ahc_max_amp_df = pd.DataFrame(\n",
    "                ahc_max_amp_array)  #these lines are new - this line of code produces the correct values but does not add them into an appended dataframe. which is obviously a problem. the task is to get these and all the values from the other two measures into a single dataframe so that in the future we may compare across groups within python<3\n",
    "            ahc_max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "            ahc_max_amp_df['genotype'] = genotype  #adding a column to add the genotype\n",
    "            ahc_append.append(ahc_max_amp_df)  #Here!\n",
    "            ahc_max_amp_concat_df = pd.concat(ahc_append)  #This is the variable for ahc\n",
    "\n",
    "            #Block of code for AHC AUC\n",
    "            averaged_sk_trace_as_np = averaged_sk_trace.to_numpy()\n",
    "            flattened_average_sk_trace = np.ndarray.flatten(averaged_sk_trace_as_np)\n",
    "            ahc_auc = np.trapz(flattened_average_sk_trace)  #this is the auc of the whole ahc\n",
    "            #bit of code to get the ahc auc into readable condition\n",
    "            ahc_auc_array = np.array(ahc_auc, ndmin=2)\n",
    "            ahc_auc_df = pd.DataFrame(ahc_auc_array)  #here we have the auc data in a dataframe\n",
    "            ahc_auc_df['file_name'] = file_name\n",
    "            ahc_auc_append.append(ahc_auc_df)\n",
    "            ahc_auc_concat_df = pd.concat(ahc_auc_append)  #this is the variable for auc\n",
    "\n",
    "            #Block of code for kinetics\n",
    "            trace_for_kinetics = flattened_average_sk_trace[30:]\n",
    "            times_for_kinetics = flat_times[30:]\n",
    "            trace_for_kinetics_pd = pd.DataFrame(trace_for_kinetics)\n",
    "            #trace_for_kinetics_pd.to_excel(\"trace_for_kinetics_pd.xlsx\")\n",
    "            times_for_kinetics_pd = pd.DataFrame(times_for_kinetics)\n",
    "            #times_for_kinetics_pd.to_excel(\"times_for_kinetics_pd.xlsx\")\n",
    "\n",
    "            #fit the curve for inactivation tau\n",
    "            p0 = [1.187 * 10 ** 150., .02, +16]  #values near what we expect   #here\n",
    "            params, cv = scipy.optimize.curve_fit(monoExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "                                                  bounds=(-np.inf, np.inf),\n",
    "                                                  maxfev=100000)  #here  #this fits the training curve with an r-squared of 0.97\n",
    "            m, t, b = params  #here\n",
    "            #m, t = params\n",
    "            sampleRate = 20_000  #hz\n",
    "            tauSec = (1 / t) / sampleRate\n",
    "\n",
    "            #determine quality of fit\n",
    "            squaredDiffs = np.square(trace_for_kinetics - monoExp(times_for_kinetics, m, t, b))  #here\n",
    "            squaredDiffsFromMean = np.square(trace_for_kinetics - np.mean(trace_for_kinetics))\n",
    "            rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "            #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "            #plot results\n",
    "            #plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "            #plt.plot(times_for_kinetics, monoExp(times_for_kinetics, m, t, b), '--', label=\"fitted\")  #here\n",
    "            plt.show()\n",
    "            #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "            #inspect the params\n",
    "            #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "            #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "            plt.show()\n",
    "            tau_flat_ms = tauSec * 1e4\n",
    "\n",
    "            #Bit of code to get tau into working order\n",
    "            tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "            tau_df = pd.DataFrame(tau_array)\n",
    "            tau_df['file_name'] = file_name\n",
    "            tau_append.append(tau_df)\n",
    "            tau_concat_df = pd.concat(tau_append)  #this is the variable for tau\n",
    "\n",
    "    #lets rename columns and export to excel for each of our metrics\n",
    "    ahc_max_amp_concat_df.rename(columns={0: 'SK Max_Amplitude (pA)'}, inplace=True)\n",
    "    ahc_max_amp_concat_df.to_excel('sk_max_amp_' + '.xlsx', index=False)\n",
    "\n",
    "    ahc_auc_concat_df.rename(columns={0: 'SK AUC (pA*s)'}, inplace=True)\n",
    "    ahc_auc_concat_df.to_excel('sk_auc' + '.xlsx', index=False)\n",
    "\n",
    "    tau_concat_df.rename(columns={0: 'SK Decay Tau (ms)'}, inplace=True)\n",
    "    tau_concat_df.to_excel('sk_tau' + '.xlsx', index=False)\n",
    "\n",
    "    return display(ahc_max_amp_concat_df), display(ahc_auc_concat_df), display(tau_concat_df)\n",
    "\n",
    "\n",
    "def generalized_v_clamp_analysis(path, sampling_rate_hz, analysis_time_start_ms, analysis_time_end_ms,\n",
    "                                 baseline_start_ms, baseline_end_ms, time_from_analysis_start_time_for_decay_tau):\n",
    "    os.chdir(path)\n",
    "    ahc_append = []\n",
    "    ahc_auc_append = []\n",
    "    tau_append = []\n",
    "\n",
    "    def monoExp(x, m, t, b):\n",
    "        return m * np.exp(-t * x) + b\n",
    "\n",
    "    #here we need to convert sampling rate to time in ms, and as integers, otherwise we cannot slice with them\n",
    "    analysis_time_start1 = int(sampling_rate_hz * analysis_time_start_ms)\n",
    "    analysis_time_end1 = int(sampling_rate_hz * analysis_time_end_ms)\n",
    "    baseline_start1 = int(sampling_rate_hz * baseline_start_ms)\n",
    "    baseline_end1 = int(sampling_rate_hz * baseline_end_ms)\n",
    "    time_from_analysis_start_time_for_decay_tau1 = int(sampling_rate_hz * time_from_analysis_start_time_for_decay_tau)\n",
    "    for file_name in os.listdir():\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    #time1_slice = slice(analysis_time_start1:analysis_time_end1)\n",
    "                    times1 = data['T'][analysis_time_start1:analysis_time_end1]  #this is 16.101 to 16.213\n",
    "                    amps1 = data['A'][analysis_time_start1:analysis_time_end1]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][\n",
    "                                       baseline_start1:baseline_end1])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    amps1_df = pd.DataFrame(amps1)\n",
    "                    flat_times = np.ndarray.flatten(times1)\n",
    "                    tabla.append(amps1_df)\n",
    "                amps_concat = pd.concat(tabla, axis=1)\n",
    "            averaged_sk_trace = amps_concat.mean(axis=1)  #woohooo this is the averaged SK trace in a pd.DF\n",
    "\n",
    "            #Block of code for AHC Max Amplitude in pA\n",
    "            ahc_max_amp = pd.DataFrame.max(averaged_sk_trace)  #max of the whole ahc, not just sk component\n",
    "            ahc_max_amp_array = np.array(ahc_max_amp, ndmin=2)  #these lines are new\n",
    "            ahc_max_amp_df = pd.DataFrame(ahc_max_amp_array)  #these lines are new\n",
    "            ahc_max_amp_df['file_name'] = file_name\n",
    "            ahc_append.append(ahc_max_amp_df)  #Here!\n",
    "            ahc_max_amp_concat_df = pd.concat(ahc_append)\n",
    "\n",
    "            #Block of code for AUC\n",
    "            averaged_sk_trace_as_np = averaged_sk_trace.to_numpy()\n",
    "            flattened_average_sk_trace = np.ndarray.flatten(averaged_sk_trace_as_np)\n",
    "            ahc_auc = np.trapz(flattened_average_sk_trace)  #this is the auc of the whole ahc\n",
    "            #bit of code to get the ahc auc into readable condition\n",
    "            ahc_auc_array = np.array(ahc_auc, ndmin=2)\n",
    "            ahc_auc_df = pd.DataFrame(ahc_auc_array)  #here we have the auc data in a dataframe\n",
    "            ahc_auc_df['file_name'] = file_name\n",
    "            ahc_auc_append.append(ahc_auc_df)\n",
    "            ahc_auc_concat_df = pd.concat(ahc_auc_append)\n",
    "\n",
    "            #Block of code for decay tau\n",
    "            trace_for_kinetics = flattened_average_sk_trace[time_from_analysis_start_time_for_decay_tau1:]\n",
    "            times_for_kinetics = flat_times[time_from_analysis_start_time_for_decay_tau1:]\n",
    "            trace_for_kinetics_pd = pd.DataFrame(trace_for_kinetics)\n",
    "            #trace_for_kinetics_pd.to_excel(\"trace_for_kinetics_pd.xlsx\")\n",
    "            times_for_kinetics_pd = pd.DataFrame(times_for_kinetics)\n",
    "            #times_for_kinetics_pd.to_excel(\"times_for_kinetics_pd.xlsx\")\n",
    "\n",
    "            #fit the curve for inactivation tau\n",
    "            p0 = [1.187 * 10 ** 150., .02, +16]  #values near what we expect   #here\n",
    "            params, cv = scipy.optimize.curve_fit(monoExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "                                                  bounds=(-np.inf, np.inf),\n",
    "                                                  maxfev=100000)  #here  #this fits the training curve with an r-squared of 0.97\n",
    "            m, t, b = params  #here\n",
    "            #m, t = params\n",
    "            sampleRate = sampling_rate_hz  #hz\n",
    "            tauSec = (1 / t) / sampleRate\n",
    "\n",
    "            #determine quality of fit\n",
    "            squaredDiffs = np.square(trace_for_kinetics - monoExp(times_for_kinetics, m, t, b))  #here\n",
    "            squaredDiffsFromMean = np.square(trace_for_kinetics - np.mean(trace_for_kinetics))\n",
    "            rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "            #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "            #plot results\n",
    "            #plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "            #plt.plot(times_for_kinetics, monoExp(times_for_kinetics, m, t, b), '--', label=\"fitted\")  #here\n",
    "            plt.show()\n",
    "            #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "            #inspect the params\n",
    "            #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "            #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "            plt.show()\n",
    "            tau_flat_ms = tauSec * 1e4\n",
    "            tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "            tau_df = pd.DataFrame(tau_array)\n",
    "            tau_df['file_name'] = file_name\n",
    "            tau_append.append(tau_df)\n",
    "            tau_concat_df = pd.concat(tau_append)\n",
    "    #for file_name_1 in os.listdir():\n",
    "    #if file_name_1.endswith(\".axgd\") or file_name_1.endswith(\".axgx\"):\n",
    "    #filename = file_name_1\n",
    "\n",
    "    #Block of code to add a column containing the file name for each metric, change the column name, and print to excel\n",
    "    #ahc_max_amp_concat_df['File'] = filename\n",
    "    ahc_max_amp_concat_df.rename(columns={0: 'Max Amplitude(pA)'}, inplace=True)\n",
    "    ahc_max_amp_concat_df.to_excel('Gen_' + 'ahc_max_amp' + '.xlsx', index=False)\n",
    "    #bit of tricky code for the auc\n",
    "\n",
    "    #ahc_auc_concat_df['File'] = filename\n",
    "    ahc_auc_concat_df.rename(columns={0: 'AUC (pA*s)'}, inplace=True)\n",
    "    ahc_auc_concat_df.to_excel('Gen_' + 'ahc_auc' + '.xlsx', index=False)\n",
    "\n",
    "    #same bit for the decay tau\n",
    "    #tau_concat_df['File'] = filename\n",
    "    tau_concat_df.rename(columns={0: 'Tau (ms)'}, inplace=True)\n",
    "    tau_concat_df.to_excel('Gen_' + 'ahc_tau' + '.xlsx', index=False)\n",
    "    return display(ahc_max_amp_concat_df), display(ahc_auc_concat_df), display(tau_concat_df)\n",
    "\n",
    "\n",
    "def AHP(path, sweep):\n",
    "    ahp_array = []\n",
    "    os.chdir(path)\n",
    "    for file_name in os.listdir():\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            file = load_neo_file(file_name)\n",
    "            for traces in file:\n",
    "                for data in traces:\n",
    "                    times1 = data['T'][15400:20000]\n",
    "                    volts1 = data['V'][15400:20000]\n",
    "                    baseline = data['V'][3500:4700]\n",
    "                    min_volts = np.min(volts1)\n",
    "                    ahp_min = baseline - min_volts\n",
    "                    max_ahp = ahp_min[sweep]\n",
    "\n",
    "            ahp_array.append(max_ahp)\n",
    "    ahp_df = pd.DataFrame(ahp_array)\n",
    "    ahp_df.to_excel('AHP_min' + '.xlsx', index=False)\n",
    "    #ahp_concat = pd.concat(ahp_array, ignore_index=True, axis=0)\n",
    "    return display(ahp_df)\n",
    "\n",
    "\n",
    "#time to peak for AHC current\n",
    "def time_to_peak(path, sampling_rate_hz, analysis_time_start_ms, analysis_time_end_ms, baseline_start_ms,\n",
    "                 baseline_end_ms, time_to_add_to_time_to_peak_in_ms):\n",
    "    os.chdir(path)\n",
    "    time_to_peak_append = []\n",
    "\n",
    "    #here we need to convert sampling rate to time in ms, and as integers, otherwise we cannot slice with them\n",
    "    analysis_time_start1 = int(sampling_rate_hz * analysis_time_start_ms)\n",
    "    analysis_time_end1 = int(sampling_rate_hz * analysis_time_end_ms)\n",
    "    baseline_start1 = int(sampling_rate_hz * baseline_start_ms)\n",
    "    baseline_end1 = int(sampling_rate_hz * baseline_end_ms)\n",
    "\n",
    "    for file_name in os.listdir():\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    #time1_slice = slice(analysis_time_start1:analysis_time_end1)\n",
    "                    times1 = data['T'][analysis_time_start1:analysis_time_end1]  #this is 16.101 to 16.213\n",
    "                    amps1 = data['A'][analysis_time_start1:analysis_time_end1]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][\n",
    "                                       baseline_start1:baseline_end1])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    amps1_df = pd.DataFrame(amps1)\n",
    "                    flat_times = np.ndarray.flatten(times1)\n",
    "                    tabla.append(amps1_df)\n",
    "                amps_concat = pd.concat(tabla, axis=1)\n",
    "            averaged_sk_trace = amps_concat.mean(axis=1)  #woohooo this is the averaged SK trace in a pd.DF\n",
    "            averaged_sk_trace_df = pd.DataFrame(averaged_sk_trace)\n",
    "            #copy the above code to generate a time to peak for the AHC current, or any current you like\n",
    "            index_of_max = pd.DataFrame.idxmax(averaged_sk_trace_df)\n",
    "            ms_from_start_analysis_to_max = ((\n",
    "                                                         index_of_max / sampling_rate_hz) * 1e3) + time_to_add_to_time_to_peak_in_ms  #lets think about this line of code\n",
    "            ms_from_start_analysis_to_max_array = np.array(ms_from_start_analysis_to_max)\n",
    "            time_to_peak_df = pd.DataFrame(ms_from_start_analysis_to_max_array)\n",
    "            time_to_peak_df['file_name'] = file_name\n",
    "            time_to_peak_append.append(time_to_peak_df)\n",
    "            time_to_peak_concat_df = pd.concat(time_to_peak_append)\n",
    "    #time_to_peak_concat_df['File'] = file_name\n",
    "    time_to_peak_concat_df.rename(columns={0: 'Time to Peak (ms)'}, inplace=True)\n",
    "    time_to_peak_concat_df.to_excel('Gen_' + 'time_to_peak' + '.xlsx', index=False)\n",
    "\n",
    "    return display(time_to_peak_concat_df)\n",
    "\n",
    "\n",
    "#Cell attached analysis which takes the path to the directory holding the action current traces\n",
    "def cell_attached_analysis_2(path, genotype):\n",
    "    os.chdir(path)\n",
    "    concat_cv_append = []\n",
    "    concat_frequency_append = []\n",
    "    for file_name in os.listdir():\n",
    "        cv_of_isi_append = []\n",
    "        frequency_append = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for cell_attached_data in traces:\n",
    "                for data in cell_attached_data:\n",
    "                    times1 = data['T']\n",
    "                    amps1 = data['A']\n",
    "                    baseline = np.average(data['A'])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    #we generated all the amps into a dataframe, check\n",
    "                    sampling_rate = 50_000\n",
    "                    #Filter the signal (savgol)\n",
    "                    window_length = 75\n",
    "                    deriv = 0\n",
    "                    polyorder = 2\n",
    "                    current_filtered = signal.savgol_filter(x=amps1, window_length=window_length, polyorder=polyorder,\n",
    "                                                            deriv=deriv, axis=0,\n",
    "                                                            cval='nearest')  #convolve with a small window, low order polynomial\n",
    "                    amps1_df = pd.DataFrame(current_filtered)\n",
    "                    std_trace = np.std(current_filtered)\n",
    "\n",
    "                    flat_amps_concat_np = current_filtered.flatten('F')\n",
    "                    flat_amps_concat_np_df = pd.DataFrame(flat_amps_concat_np)\n",
    "                    median_trace = np.median(flat_amps_concat_np)\n",
    "                    flat_times = times1.flatten('F')\n",
    "                    if std_trace < 3.5:\n",
    "                        width = 0.9 * sampling_rate / 1000\n",
    "                        peaks, peaks_dict = find_peaks(-flat_amps_concat_np,  # signal\n",
    "                                                       height=(1.3 * std_trace, 200),\n",
    "                                                       # Min and max thresholds to detect peaks.\n",
    "                                                       threshold=None,\n",
    "                                                       # Min and max vertical distance to neighboring samples.\n",
    "                                                       distance=(1000),  # Min horizontal distance between peaks.\n",
    "                                                       prominence=12,\n",
    "                                                       # Vertical distance between the peak and lowest contour line.\n",
    "                                                       width=width,\n",
    "                                                       # Min required width (in bins). E.g. For 50Khz, 10 bins = 5 ms.\n",
    "                                                       wlen=None,  # Window length to calculate prominence.\n",
    "                                                       rel_height=1.0,\n",
    "                                                       # Relative height at which the peak width is measured.\n",
    "                                                       plateau_size=None)\n",
    "                        # plt.figure(figsize=(16, 8))\n",
    "                        # plt.plot(flat_amps_concat_np)\n",
    "                        # plt.plot(peaks, flat_amps_concat_np[peaks], \"x\")\n",
    "                        # plt.show()\n",
    "                        if len(peaks) > 2:\n",
    "                            isi_s = np.diff(peaks, axis=0, prepend=peaks[0])[1:] / sampling_rate\n",
    "                            isis_pd = pd.DataFrame(isi_s)\n",
    "                            isi_std = np.std(isi_s)\n",
    "                            isi_mean = np.mean(isi_s)\n",
    "                            cv_of_isi = isi_std / isi_mean\n",
    "                            cv_array = np.array(cv_of_isi)\n",
    "                            #frequency based on ISIs\n",
    "                            frequency = np.mean(1 / isi_s)\n",
    "                            frequency_based_on_time = len(peaks) / 10\n",
    "                            cv_of_isi_append.append(cv_of_isi)\n",
    "                            frequency_append.append(frequency)\n",
    "            mean_cv = np.mean(cv_of_isi_append)\n",
    "            mean_cv_array = np.array(mean_cv, ndmin=2)\n",
    "            mean_cv_df = pd.DataFrame(mean_cv_array)\n",
    "            mean_cv_df['file_name'] = file_name\n",
    "            mean_cv_df['Genotype'] = genotype\n",
    "            concat_cv_append.append(mean_cv_df)\n",
    "            mean_frequency = np.mean(frequency_append)\n",
    "            mean_frequency_array = np.array(mean_frequency, ndmin=2)\n",
    "            mean_frequency_df = pd.DataFrame(mean_frequency_array)\n",
    "            mean_frequency_df['file_name'] = file_name\n",
    "            mean_frequency_df['Genotype'] = genotype\n",
    "            concat_frequency_append.append(mean_frequency_df)\n",
    "    cv_append_concat = pd.concat(concat_cv_append)\n",
    "    cv_append_concat.rename(columns={0: 'CV of ISI'}, inplace=True)\n",
    "    cv_append_concat.to_excel('CV of ISI' + '.xlsx', index=False)\n",
    "    frequency_append_concat = pd.concat(concat_frequency_append)\n",
    "    frequency_append_concat.rename(columns={0: 'Mean Frequency (Hz)'}, inplace=True)\n",
    "    frequency_append_concat.to_excel('Cell Attached Frequency (Hz)' + '.xlsx', index=False)\n",
    "    return display(cv_append_concat), display(frequency_append_concat)\n",
    "\n",
    "\n",
    "def input_resistance(path, sweep_start, sweep_end):\n",
    "    #we are going to define input resistance as voltage_deflection_vb_ssse / stimulus_current, because we cannot extract stimulus current from the axograph metadata, we need to build a column in a pandas dataframe, and then we need to divide those two columns providing us a measure.\n",
    "    table2 = []\n",
    "    feature = 'voltage_deflection_vb_ssse'\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        table = pd.DataFrame(columns=[feature])  #create a table that has columns with the name you want\n",
    "        table.name = feature  #the tables name\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):  #check for the filetype\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500,\n",
    "                                             stim_end=1500)  #load the trace, and define stim start and stop\n",
    "            for data in traces[sweep_start:sweep_end]:  #loop through these guys\n",
    "                #table.rename(columns={feature:filename}, inplace=True) #renaming the columns with the correct file !\n",
    "                feature_values = efel.getFeatureValues(data, [feature], raise_warnings=None)[\n",
    "                    0]  #this is the feature extraction\n",
    "                if feature_values[feature] is not None:\n",
    "                    # Define the parameters for detection\n",
    "                    efel.api.setThreshold(-10)  # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20)  # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature][0]\n",
    "\n",
    "            Current_injected = np.linspace(-50.0, -10, num=5)\n",
    "            current_injected_df = pd.DataFrame(Current_injected)\n",
    "\n",
    "            table2.append(table)\n",
    "\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "\n",
    "            table.rename(columns={feature: feature}, inplace=True)  #renaming the columns with the correct file !\n",
    "\n",
    "        #block of code to combine all of the generated excel workbooks into a single workbook\n",
    "        #df_concat.to_excel(feature + 'master_file.xlsx', index=False)\n",
    "    Current_injected = np.linspace(-50.0, -10, num=5)\n",
    "    table2 = df_concat.assign(Current_injected=Current_injected)\n",
    "    voltage_deflection_vb_ssse_values = np.array(table2['voltage_deflection_vb_ssse'])\n",
    "    current_injected_values = np.array(table2['Current_injected'])\n",
    "    voltage_values_df = pd.DataFrame(voltage_deflection_vb_ssse_values)\n",
    "    #option 1 - this divides the voltage deflec values by the injected current, i like this option less\n",
    "    input_resistances = []\n",
    "    for column in voltage_values_df.columns:\n",
    "        input_resistance = (voltage_values_df[column]) / Current_injected\n",
    "        input_resistance_array = np.asarray(input_resistance)\n",
    "        input_resistance_df = pd.DataFrame(input_resistance_array)\n",
    "        input_resistances.append(input_resistance_df)\n",
    "\n",
    "    input_resistances_concat = pd.concat(input_resistances, axis=1)\n",
    "    input_resistances_concat.rename(columns={0: filename}, inplace=True)\n",
    "\n",
    "    #option 2 - this is the lin regress fit to generate the input resistance, which I like better\n",
    "    slope_list = []\n",
    "    for column in voltage_values_df.columns:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(Current_injected.astype(float),\n",
    "                                                                       (voltage_values_df[column].astype(float)))\n",
    "        slope_array = np.array(slope, ndmin=2) * 1000\n",
    "        slope_df = pd.DataFrame(slope_array)\n",
    "\n",
    "        slope_list.append(slope_df)\n",
    "\n",
    "    slope_concat = pd.concat(slope_list, axis=0)\n",
    "    slope_concat.rename(columns={0: 'filename'}, inplace=True)\n",
    "    slope_concat.to_excel('Input Resistance' + '.xlsx', index=False)\n",
    "\n",
    "    return display(slope_concat)\n",
    "\n",
    "\n",
    "#capacitance\n",
    "def capacitance(path, sweep_start, sweep_end):\n",
    "    table2 = []\n",
    "    feature = 'decay_time_constant_after_stim'\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        table = pd.DataFrame(columns=[feature])  #create a table that has columns with the name you want\n",
    "        table.name = feature  #the tables name\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):  #check for the filetype\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500,\n",
    "                                             stim_end=1500)  #load the trace, and define stim start and stop\n",
    "            for data in traces[9:10]:  #loop through these guys\n",
    "                #table.rename(columns={feature:filename}, inplace=True) #renaming the columns with the correct file !\n",
    "                feature_values = efel.getFeatureValues(data, [feature], raise_warnings=None)[\n",
    "                    0]  #this is the feature extraction\n",
    "                if feature_values[feature] is not None:\n",
    "                    # Define the parameters for detection\n",
    "                    efel.api.setThreshold(-10)  # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20)  # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature][0]\n",
    "\n",
    "            Current_injected = np.linspace(-50.0, -10, num=5)\n",
    "            current_injected_df = pd.DataFrame(Current_injected)\n",
    "            table2.append(table)\n",
    "            membrane_time_constant_concat = pd.concat(table2, axis=0)\n",
    "\n",
    "    membrane_time_constant_array = np.array(membrane_time_constant_concat)\n",
    "\n",
    "    table3 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        feature = 'voltage_deflection_vb_ssse'\n",
    "        table = pd.DataFrame(columns=[feature])  #create a table that has columns with the name you want\n",
    "        table.name = feature  #the tables name\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):  #check for the filetype\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500,\n",
    "                                             stim_end=1500)  #load the trace, and define stim start and stop\n",
    "            for data in traces[sweep_start:sweep_end]:  #loop through these guys\n",
    "                #table.rename(columns={feature:filename}, inplace=True) #renaming the columns with the correct file !\n",
    "                feature_values = efel.getFeatureValues(data, [feature], raise_warnings=None)[\n",
    "                    0]  #this is the feature extraction\n",
    "                if feature_values[feature] is not None:\n",
    "                    # Define the parameters for detection\n",
    "                    efel.api.setThreshold(-10)  # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20)  # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature][0]\n",
    "\n",
    "            Current_injected = np.linspace(-60.0, -10, num=6)\n",
    "            current_injected_df = pd.DataFrame(Current_injected)\n",
    "\n",
    "            table3.append(table)\n",
    "\n",
    "            df_concat = pd.concat(table3, axis=1)\n",
    "\n",
    "            table.rename(columns={feature: feature}, inplace=True)  #renaming the columns with the correct file !\n",
    "\n",
    "        #block of code to combine all of the generated excel workbooks into a single workbook\n",
    "        #df_concat.to_excel(feature + 'master_file.xlsx', index=False)\n",
    "    Current_injected = np.linspace(-60.0, -10, num=6)\n",
    "    table3 = df_concat.assign(Current_injected=Current_injected)\n",
    "    voltage_deflection_vb_ssse_values = np.array(table3['voltage_deflection_vb_ssse'])\n",
    "    current_injected_values = np.array(table3['Current_injected'])\n",
    "    voltage_values_df = pd.DataFrame(voltage_deflection_vb_ssse_values)\n",
    "    #option 1\n",
    "    input_resistances = []\n",
    "    for column in voltage_values_df.columns:\n",
    "        input_resistance = (voltage_values_df[column]) / Current_injected\n",
    "        input_resistance_array = np.asarray(input_resistance)\n",
    "        input_resistance_df = pd.DataFrame(input_resistance_array)\n",
    "        input_resistances.append(input_resistance_df)\n",
    "\n",
    "    input_resistances_concat = pd.concat(input_resistances, axis=1)\n",
    "    input_resistances_concat.rename(columns={0: filename}, inplace=True)\n",
    "\n",
    "    #option 2 - able to get the values, now how do i put them together ???\n",
    "    slope_list = []\n",
    "    for column in voltage_values_df.columns:\n",
    "        #slope, intercept, r_value, p_value, std_err = stats.linregress(Current_injected.astype(float),(voltage_values_df[column].astype(float)))\n",
    "        m, b = np.polyfit(Current_injected.astype(float), (voltage_values_df[column].astype(float)), 1)\n",
    "        slope_array = np.array(m, ndmin=2) * 1000\n",
    "        slope_df = pd.DataFrame(slope_array)\n",
    "        slope_list.append(slope_df)\n",
    "\n",
    "    slope_concat = pd.concat(slope_list, axis=0)\n",
    "    slope_concat_array = np.array(slope_concat)\n",
    "    slope_concat.rename(columns={0: 'Input Resistance'}, inplace=True)\n",
    "\n",
    "    capacitance_final = (membrane_time_constant_array / slope_concat_array) * 1000\n",
    "    cap_final_df = pd.DataFrame(capacitance_final)\n",
    "    cap_final_df.to_excel('Input Resistance' + '.xlsx', index=False)\n",
    "\n",
    "    return display(cap_final_df), display(slope_concat)\n",
    "\n",
    "\n",
    "def h_current_analysis(path, genotype):\n",
    "    ahc_append = []\n",
    "    sampling_rate = 10000\n",
    "    os.chdir(path)\n",
    "    tabla = []\n",
    "    for file_name in os.listdir():\n",
    "\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for h_data in traces:\n",
    "\n",
    "                for data in h_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][5140:6263]  #this is\n",
    "                    amps1 = data['A'][5140:6263]\n",
    "                    times2 = data['T'][14606:14950]\n",
    "                    amps2 = data['A'][14606:14950]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][4760:4943])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first portion\n",
    "                    amps2 = amps2 - baseline  #subtract the baseline from the amplitudes of the second portion\n",
    "                    max_amps1 = np.max(amps1)  #find the max of the first portion\n",
    "                    min_amps2 = np.min(amps2)  #find the min of the second portion\n",
    "                h_current = max_amps1 - min_amps2  #subtract the min from the max to get the h current\n",
    "                h_current_np = np.array(h_current, ndmin=2)\n",
    "                h_current_df = pd.DataFrame(h_current_np)\n",
    "                h_current_df['file_name'] = file_name\n",
    "                h_current_df['Genotype'] = genotype\n",
    "                tabla.append(h_current_df)\n",
    "    h_current_concat = pd.concat(tabla, axis=0)\n",
    "    h_current_concat.rename(columns={0: 'Amplitude (pA)'}, inplace=True)\n",
    "    h_current_concat.to_excel('H Current_amplitude' + '.xlsx', index=False)\n",
    "    return display(h_current_concat)\n",
    "\n",
    "\n",
    "def collect_isis_global_excel(path1, sample_rate, sweep_start, sweep_end, time_start_s, time_end_s, metadata):\n",
    "    os.chdir(path1)\n",
    "    num_points = 1000\n",
    "    sampling_rate = sample_rate\n",
    "    time_start = time_start_s * sampling_rate\n",
    "    time_end = time_end_s * sampling_rate\n",
    "    global_mean_traces = []\n",
    "    global_mean_isi = []\n",
    "    filename_list = []\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "\n",
    "    for filename in file_names:\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            filename_list.append(filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            if len(traces) > 0:\n",
    "                file_results = []\n",
    "                average_isi = []\n",
    "                for trace in traces:\n",
    "                    for p in trace:\n",
    "                        times = (p['T'])\n",
    "                        times = times[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        voltages = (p['V']).flatten()\n",
    "                        voltages = voltages[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        times -= times[0]\n",
    "                        dt = times[2] - times[1]\n",
    "                        detection_level = -20\n",
    "                        min_interval = 0.0001\n",
    "                        spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                        interspike_intervals = np.diff(spike_times)\n",
    "                        if len(interspike_intervals) > 0:\n",
    "                            avg_isi_1 = np.mean(interspike_intervals)\n",
    "                            average_isi.append(avg_isi_1)\n",
    "                            resampled_isi_voltage_arrays = []\n",
    "                            for i in range(len(interspike_intervals)):\n",
    "                                start_time = spike_times[i]\n",
    "                                end_time = spike_times[i] + interspike_intervals[i]\n",
    "                                times_rel = np.linspace(0, 1, num_points)\n",
    "                                isi_voltage_array = segment(voltages, dt, start_time, end_time)\n",
    "                                resampled_isi_voltage_arrays.append(\n",
    "                                    np.interp(times_rel, np.linspace(0, 1, len(isi_voltage_array)), isi_voltage_array))\n",
    "                            mean_resampled_isi_voltages = np.mean(\n",
    "                                np.concatenate(resampled_isi_voltage_arrays).reshape(len(resampled_isi_voltage_arrays),\n",
    "                                                                                     num_points), axis=0)\n",
    "                            file_results.append(mean_resampled_isi_voltages)\n",
    "            isi_file_mean = np.mean(np.array(average_isi))\n",
    "            global_mean_isi.append(isi_file_mean)\n",
    "            file_mean = np.mean(np.array(file_results), axis=0)\n",
    "            global_mean_traces.append(file_mean)\n",
    "    filename_df = pd.DataFrame(filename_list)\n",
    "    filename_df.rename(columns={0: 'filename'}, inplace=True)\n",
    "    global_mean_isis = np.array(global_mean_isi)\n",
    "    global_mean_isi_df = pd.DataFrame(global_mean_isis)\n",
    "    global_mean_isi_df = pd.concat([filename_df, global_mean_isi_df], axis=1)\n",
    "    global_mean_isi_df.set_index('filename', inplace=True)\n",
    "    global_mean_isi_df.sort_index(inplace=True)\n",
    "    global_mean_isis = global_mean_isi_df.values[:, 0]\n",
    "    global_mean_isi_df.to_excel(metadata + '_global_mean_isi.xlsx')\n",
    "    isi_final_mean = np.mean(global_mean_isi, axis=0)\n",
    "    isi_final_mean = np.array([[isi_final_mean]])\n",
    "    isi_final_mean_df = pd.DataFrame(isi_final_mean)\n",
    "    display(isi_final_mean_df)\n",
    "    isi_final_mean_df.to_excel(metadata + '_isi_final_mean.xlsx')\n",
    "\n",
    "    global_mean_traces = np.array(global_mean_traces)\n",
    "    global_mean_traces_df = pd.DataFrame(global_mean_traces)\n",
    "    global_mean_traces_df = pd.concat([filename_df, global_mean_traces_df], axis=1)\n",
    "    global_mean_traces_df.set_index('filename', inplace=True)\n",
    "    global_mean_traces_df.sort_index(inplace=True)\n",
    "    global_mean_traces = np.array(global_mean_traces_df)\n",
    "    global_mean_traces_df.to_excel(metadata + '_global_mean_traces.xlsx')\n",
    "\n",
    "    final_mean_trace = np.mean(global_mean_traces, axis=0)\n",
    "    final_mean_trace_df = pd.DataFrame(final_mean_trace)\n",
    "    final_mean_trace_df.to_excel(metadata + '_final_mean_trace.xlsx')\n",
    "\n",
    "    file_results = np.array(file_results)\n",
    "    file_results_df = pd.DataFrame(file_results)\n",
    "    file_results_df = pd.concat([filename_df, file_results_df], axis=1)\n",
    "    file_results_df.set_index('filename', inplace=True)\n",
    "    file_results_df.sort_index(inplace=True)\n",
    "    file_results_df.to_excel(metadata + '_total_trajectories.xlsx')\n",
    "    print('Analysis complete')\n",
    "    return final_mean_trace, isi_final_mean, global_mean_traces, file_results, global_mean_isis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_trajectory_independent(mean_trace, avg_isi, total_trajectory_voltages, color):\n",
    "    dVdt = np.gradient(mean_trace, axis=0)\n",
    "    slope = np.mean(dVdt / 0.1)\n",
    "    avg_isis = np.linspace(0, avg_isi * len(mean_trace), len(mean_trace))\n",
    "    ci = 1.96 * np.std(np.array(total_trajectory_voltages), axis=0) / np.sqrt(len(total_trajectory_voltages))\n",
    "\n",
    "    # calculate duration of each trace\n",
    "    duration = len(mean_trace) * avg_isi\n",
    "\n",
    "    # calculate scaling factor for x-axis\n",
    "    scale = duration / len(mean_trace)\n",
    "\n",
    "    # plot mean trace with actual duration\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(0, duration, scale), mean_trace, color=color, label='Mean Trace')\n",
    "    #ax.fill_between(np.arange(0, duration, scale), mean_trace - ci, mean_trace + ci, alpha=0.3, color='#848482', label='95% CI')\n",
    "    ax.set_xlabel('Time from spike (ms)')\n",
    "    ax.set_ylabel('Membrane Potential (mV)')\n",
    "    ax.set_title('ISI voltage trajectories')\n",
    "\n",
    "    ax.legend(loc='lower right', borderpad=0.1, labelspacing=.3)\n",
    "    plt.ylim(-65, -30)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_voltage_trajectories_two(mean_trace1, mean_trace2, avg_isi1, avg_isi2, total_trajectory_voltages1, total_trajectory_voltages2, mean_trace1_label, mean_trace2_label, mean_trace1_color, mean_trace2_color, save, metadata):\n",
    "\n",
    "    # Calculate first baseline statistics\n",
    "    dVdt1 = np.gradient(mean_trace1, axis=0)\n",
    "    slope1 = np.mean(dVdt1 / 0.1)\n",
    "    avg_isis1 = np.linspace(0, avg_isi1 * len(mean_trace1), len(mean_trace1))\n",
    "    #ci1 = 1.96 * np.std(np.array(total_trajectory_voltages1), axis=0) / np.sqrt(len(total_trajectory_voltages1))\n",
    "\n",
    "    # Calculate second baseline statistics\n",
    "    dVdt2 = np.gradient(mean_trace2, axis=0)\n",
    "    slope2 = np.mean(dVdt2 / 0.1)\n",
    "    avg_isis2 = np.linspace(0, avg_isi2 * len(mean_trace2), len(mean_trace2))\n",
    "    #ci2 = 1.96 * np.std(np.array(total_trajectory_voltages2), axis=0) / np.sqrt(len(total_trajectory_voltages2))\n",
    "\n",
    "    # Calculate duration of each trace\n",
    "    num_points = 1000\n",
    "    duration1 = (len(mean_trace1) * avg_isi1) / num_points-1\n",
    "    duration2 = (len(mean_trace2) * avg_isi2) / num_points-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate scaling factors for x-axis\n",
    "    scale1 = duration1 / len(mean_trace1)\n",
    "    scale2 = duration2 / len(mean_trace2)\n",
    "\n",
    "\n",
    "    # plot mean traces with actual durations\n",
    "    fig, ax = plt.subplots(figsize=(2.75, 2.75))\n",
    "    ax.plot(np.arange(0, duration1, scale1)[:1000], mean_trace1, color=mean_trace1_color, label=mean_trace1_label, linewidth=0.5)\n",
    "    ax.plot(np.arange(0, duration2, scale2)[:1000], mean_trace2, color=mean_trace2_color, label=mean_trace2_label, linewidth=0.5)\n",
    "    # ax.fill_between(np.arange(0, duration1, scale1), mean_trace1 - ci1, mean_trace1 + ci1, alpha=0.3,\n",
    "    #                 color='#848482', label='95% CI (Trace 1)')\n",
    "    # ax.fill_between(np.arange(0, duration2, scale2), mean_trace2 - ci2, mean_trace2 + ci2, alpha=0.3,\n",
    "    #                 color='red', label='95% CI (Trace 2)')\n",
    "    ax.set_xlabel('Time from spike (ms)', fontsize=9)\n",
    "    ax.set_ylabel('Membrane Potential (mV)', fontsize=9)\n",
    "    # ax.set_title('ISI voltage trajectories')\n",
    "    ax.legend(loc='lower right', borderpad=0.4, labelspacing=.1, fontsize=7)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=8, width=0.5)  # You can adjust the font size as needed\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.50)\n",
    "    ax.spines['left'].set_linewidth(0.50)\n",
    "\n",
    "    min_value = min(np.min(mean_trace1), np.min(mean_trace2))\n",
    "    #ax.legend(fontsize=14)\n",
    "    plt.legend(frameon=False, edgecolor='none', fontsize='large', handlelength=2, handleheight=1, loc='lower right')\n",
    "    #set major ticks on the x and y axis\n",
    "\n",
    "    #x.tick_params(axis='x', labelsize=14)\n",
    "    plt.ylim(min_value-1, -30)\n",
    "\n",
    "    if save == True:\n",
    "        os.chdir('/Users/HBLANKEN/Library/CloudStorage/OneDrive-UniversityofOklahoma/Beckstead Lab/DA-AD paper files/Grand collection of axograph files/Noise Traces/Figures')\n",
    "        plt.savefig(metadata + '.pdf', dpi=600, bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "def get_first_spike_time(voltages, dt, detection_level, min_interval):\n",
    "    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "    if len(spike_times) > 0:\n",
    "        return spike_times[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def determine_first_spike_avg_sem_hyperpol_steps(path1, sample_rate, sweep_start, sweep_end, metadata):\n",
    "    os.chdir(path1)\n",
    "    results = {}\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            file_results = []\n",
    "            for i, trace in enumerate(traces):\n",
    "                for p in trace:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[30000:]  #selected for the first 10 seconds of each sweep\n",
    "                    voltages = (p['V']).flatten()\n",
    "                    voltages = voltages[30000:]  #selected for the first 10 seconds of each sweep\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    detection_level = -30\n",
    "                    min_interval = 0.0001\n",
    "                    spike_time = get_first_spike_time(voltages, dt, detection_level, min_interval)\n",
    "                    if spike_time is not None:\n",
    "                        file_results.append(spike_time)\n",
    "                if i < 10:\n",
    "                    key = f'-{i*10+100}pA'\n",
    "                else:\n",
    "                    key = f'-{i*10-100}pA'\n",
    "                results[filename + ' ' + key] = file_results\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.index.name = 'Sweep'\n",
    "    df.columns.name = 'Cell'\n",
    "    df.columns.name = 'n'\n",
    "\n",
    "    # Calculate the mean and SEM across rows\n",
    "    mean = df.mean(axis=1)\n",
    "    sem = df.sem(axis=1)\n",
    "    path = path1 + '/*.axgd' or path1 + '/*.axgx'  # replace *.txt with your desired extension\n",
    "    file_list = glob.glob(path)\n",
    "    n = len(file_list)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a new DataFrame with the mean and SEM\n",
    "    result_df = pd.DataFrame({'Mean': mean, 'SEM': sem, 'n': n, 'Metadata': metadata})\n",
    "    result_df.to_excel(metadata + 'hyperpol_steps_rebound.xlsx')\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def determine_rebound_delay_ten_x(path1, sample_rate, sweep_start, sweep_end, metadata):\n",
    "    os.chdir(path1)\n",
    "    results_mean = []\n",
    "    results_cv = []\n",
    "    num_files_analyzed = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            file_results = []\n",
    "            for trace in traces:\n",
    "                for p in trace:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[30000:]  #selected for the first 10 seconds of each sweep\n",
    "                    voltages = (p['V']).flatten()\n",
    "                    voltages = voltages[30000:]  #selected for the first 10 seconds of each sweep\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    detection_level = -30\n",
    "                    min_interval = 0.0001\n",
    "                    spike_time = get_first_spike_time(voltages, dt, detection_level, min_interval)\n",
    "                    if spike_time is not None:\n",
    "                        file_results.append(spike_time)\n",
    "            if file_results:\n",
    "                results_mean.append(np.mean(file_results)) # Average the results for the current file\n",
    "                results_cv.append(np.std(file_results) / np.mean(file_results) * 100) # Calculate the standard deviation of the results for the current file\n",
    "                num_files_analyzed.append(len(filename)) # Add the number of files analyzed for the current measure\n",
    "            else:\n",
    "                results_mean.append(np.nan)\n",
    "                results_std.append(np.nan)\n",
    "                num_files_analyzed.append(0)\n",
    "    df = pd.DataFrame({'Mean Rebound Delay': results_mean, '%Variance': results_cv, 'Metadata': metadata})\n",
    "    df.to_excel(metadata + 'ten_x_rebound_delay.xlsx')\n",
    "    return df\n",
    "\n",
    "def determine_rebound_delay_slope(path1, sample_rate, sweep_start, sweep_end, metadata):\n",
    "    os.chdir(path1)\n",
    "    results_mean = []\n",
    "    results_slope = []\n",
    "    num_files_analyzed = []\n",
    "    filename_list = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            filename_list.append(filename)\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            file_results = []\n",
    "            for trace in traces:\n",
    "                for p in trace:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[31500:]\n",
    "                    voltages = (p['V']).flatten()\n",
    "                    voltages = voltages[31500:]\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    detection_level = -30\n",
    "                    min_interval = 0.0001\n",
    "                    spike_time = get_first_spike_time(voltages, dt, detection_level, min_interval)\n",
    "                    if spike_time is not None:\n",
    "                        pre_spike_times = times[times < spike_time]\n",
    "                        pre_spike_voltages = voltages[times < spike_time]\n",
    "                        if len(pre_spike_voltages) > 2:  # Check that there are enough points to fit a line\n",
    "                            slope, intercept = np.polyfit(pre_spike_times, pre_spike_voltages, 1)\n",
    "                            file_results.append(slope)\n",
    "            if file_results:\n",
    "                results_mean.append(np.mean(file_results)) # Average the results for the current file\n",
    "                results_slope.append(np.std(file_results) / np.mean(file_results)) # Calculate the standard deviation of the results for the current file\n",
    "                num_files_analyzed.append(len(filename)) # Add the number of files analyzed for the current measure\n",
    "            else:\n",
    "                results_mean.append(np.nan)\n",
    "                results_std.append(np.nan)\n",
    "                num_files_analyzed.append(0)\n",
    "    filename_df = pd.DataFrame({'Filename': filename_list})\n",
    "    df = pd.DataFrame({'Mean Rebound Delay Slope': results_mean, 'Variance': results_slope, 'Metadata': metadata})\n",
    "    df = pd.concat([filename_df, df], axis=1)\n",
    "    df.set_index('Filename', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.to_excel(metadata + 'rebound_delay_slope.xlsx')\n",
    "    return df\n",
    "\n",
    "def calculate_slope_and_mean_voltage(global_mean_traces, global_mean_isis, save_path, metadata):\n",
    "    os.chdir(save_path)\n",
    "    slopes = []\n",
    "    mean_voltages = []\n",
    "    min_voltages = []\n",
    "    for i, trace in enumerate(global_mean_traces):\n",
    "        if len(trace) == 0:\n",
    "            print(f\"Empty trace found at index {i}\")\n",
    "            continue\n",
    "        isi = global_mean_isis[i]\n",
    "\n",
    "        # Scale the trace based on the actual length of the ISI\n",
    "        time = np.linspace(0, isi, len(trace))\n",
    "\n",
    "        slope_start_voltage_idx = int(0.3 * len(trace))  # find index of minimum voltage\n",
    "        end_idx_slope = int(0.7 * len(trace))\n",
    "        # print('Start Slope:', slope_start_voltage_idx)\n",
    "        # print('End Slope:', end_idx_slope)\n",
    "        # print('Time:', time[slope_start_voltage_idx:end_idx_slope])\n",
    "\n",
    "        min_voltage_index = np.argmin(trace)\n",
    "        mean_voltage_end_idx = int(0.98 * len(trace))\n",
    "\n",
    "        plt.plot(time[slope_start_voltage_idx:end_idx_slope], trace[slope_start_voltage_idx:end_idx_slope])\n",
    "\n",
    "        # Calculate the slope using the scaled trace\n",
    "        slope = np.gradient(trace[slope_start_voltage_idx:end_idx_slope], time[slope_start_voltage_idx:end_idx_slope]) * 1000\n",
    "\n",
    "        slopes.append(slope.mean())\n",
    "        mean_voltages.append(trace[min_voltage_index:mean_voltage_end_idx].mean())\n",
    "        min_voltages.append(trace[:].min())\n",
    "\n",
    "    slope_df = pd.DataFrame({'slope': slopes})\n",
    "    slope_df.to_excel(metadata + '_slopes.xlsx')\n",
    "    mean_voltage_df = pd.DataFrame({'mean_voltage': mean_voltages})\n",
    "    mean_voltage_df.to_excel(metadata + '_mean_voltages.xlsx')\n",
    "    min_voltage_df = pd.DataFrame({'min_voltage': min_voltages})\n",
    "    min_voltage_df.to_excel(metadata + '_min_voltages.xlsx')\n",
    "\n",
    "    results_df = pd.concat([slope_df, mean_voltage_df, min_voltage_df], axis=1)\n",
    "    return results_df\n",
    "\n",
    "def mean_thresholds(path, sweep_start, sweep_end, metadata):\n",
    "    v_threshold_list = []\n",
    "    filename_list = []\n",
    "    os.chdir(path)\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "\n",
    "    for filename in file_names:\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            filename_list.append(filename)\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=1000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            for data in traces:\n",
    "                for p in data:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[:200000]\n",
    "                    voltages = (p['V'])\n",
    "                    voltages = voltages[:200000]\n",
    "                    voltages = voltages.flatten()\n",
    "                    if len(times) >= 3:\n",
    "                        times -= times[0]\n",
    "                    if len(times) >= 3:\n",
    "                        dt = times[2] - times[1]\n",
    "                    detection_level = -24\n",
    "                    min_interval = 0.0001\n",
    "                    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                    isis = np.diff(spike_times)\n",
    "                    time_before = .018\n",
    "                    time_after = .015\n",
    "                    times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                    spike_voltages = []\n",
    "                    for i in range(0, len(spike_times)):\n",
    "                        if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                            spike_voltages.append(\n",
    "                                segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                    spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                    mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "\n",
    "            if len(mean_spike_voltages) > 1:\n",
    "                dvdt_threshold = 10\n",
    "                dvdt = find_slopes(mean_spike_voltages, dt)\n",
    "                i = 1\n",
    "                while dvdt[i] < dvdt_threshold:\n",
    "                    i += 1\n",
    "                v0 = mean_spike_voltages[i - 1]\n",
    "                v1 = mean_spike_voltages[i]\n",
    "                dvdt0 = dvdt[i - 1]\n",
    "                dvdt1 = dvdt[i]\n",
    "                v_threshold = v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0)\n",
    "\n",
    "                if not np.isnan(v_threshold) :\n",
    "                    v_threshold_list.append(v_threshold)\n",
    "                    v_max = np.max(mean_spike_voltages)\n",
    "                    v_half = (v_threshold + v_max) / 2\n",
    "    v_threshold_df = pd.DataFrame(v_threshold_list)\n",
    "    filename_df = pd.DataFrame(filename_list)\n",
    "    filename_df.rename(columns={0: 'filename'}, inplace=True)\n",
    "    v_threshold_df.rename(columns={0: 'v_threshold'}, inplace=True)\n",
    "    v_threshold_df = pd.concat([filename_df, v_threshold_df], axis=1)\n",
    "    v_threshold_df.set_index('filename', inplace=True)\n",
    "    v_threshold_df.sort_index(inplace=True)\n",
    "    v_threshold_df.to_excel(metadata + '_v_thresholds.xlsx')\n",
    "    return v_threshold_df\n",
    "\n",
    "def spikewidths_at_half_max(path, sweep_start, sweep_end, metadata):\n",
    "    os.chdir(path)\n",
    "    v_threshold_list = []\n",
    "    filename_list = []\n",
    "    spike_width_half_max_list = []\n",
    "    #sort the files in the directory\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "\n",
    "    for filename in file_names:\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            filename_list.append(filename)\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=1000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            for data in traces:\n",
    "                for p in data:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[:200000]\n",
    "                    voltages = (p['V'])\n",
    "                    voltages = voltages[:200000]\n",
    "                    voltages = voltages.flatten()\n",
    "                    if len(times) >= 3:\n",
    "                        times -= times[0]\n",
    "                    if len(times) >= 3:\n",
    "                        dt = times[2] - times[1]\n",
    "                    detection_level = -24\n",
    "                    min_interval = 0.0001\n",
    "                    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                    isis = np.diff(spike_times)\n",
    "                    time_before = .018\n",
    "                    time_after = .015\n",
    "                    times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                    spike_voltages = []\n",
    "                    for i in range(0, len(spike_times)):\n",
    "                        if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                            spike_voltages.append(\n",
    "                                segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                    spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                    mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "            if len(mean_spike_voltages) > 1:\n",
    "                dvdt_threshold = 10\n",
    "                dvdt = find_slopes(mean_spike_voltages, dt)\n",
    "\n",
    "                # Convert dvdt to a NumPy array\n",
    "                dvdt = np.array(dvdt)\n",
    "\n",
    "                # Find the first index i where dvdt[i] >= dvdt_threshold\n",
    "                i = np.argmax(dvdt >= dvdt_threshold)\n",
    "\n",
    "                # Get the voltages and dvdt values at index i-1 and i for linear interpolation\n",
    "                v0 = mean_spike_voltages[i - 1]\n",
    "                v1 = mean_spike_voltages[i]\n",
    "                dvdt0 = dvdt[i - 1]\n",
    "                dvdt1 = dvdt[i]\n",
    "\n",
    "                if abs(dvdt1 - dvdt0) > 1e-9:\n",
    "                    # Linear interpolation to find the threshold voltage v_threshold\n",
    "                    v_threshold = v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0)\n",
    "                else:\n",
    "                    v_threshold = v1\n",
    "\n",
    "                v_threshold_list.append(v_threshold)\n",
    "                v_max = np.max(mean_spike_voltages)\n",
    "                v_half = (v_threshold + v_max) / 2\n",
    "\n",
    "                # Find the index of the first occurrence where mean_spike_voltages >= v_half\n",
    "                rising_edge_idx = np.argmax(mean_spike_voltages >= v_half)\n",
    "\n",
    "                # Find the index of the first occurrence where mean_spike_voltages <= v_half\n",
    "                falling_edge_idx = np.argmin(mean_spike_voltages <= v_half)\n",
    "\n",
    "                spike_width_half_max = (times_rel[falling_edge_idx] - times_rel[rising_edge_idx]) * 100.0  # in ms\n",
    "                spike_width_half_max_list.append(spike_width_half_max)\n",
    "\n",
    "    # Create the DataFrame after the loop with the collected data\n",
    "    spike_width_half_max_df = pd.DataFrame(spike_width_half_max_list)\n",
    "    filename_df = pd.DataFrame(filename_list)\n",
    "    filename_df.rename(columns={0: 'filename'}, inplace=True)\n",
    "    spike_width_half_max_df.rename(columns={0: 'spike_width_half_max'}, inplace=True)\n",
    "    spike_width_half_max_df = pd.concat([filename_df, spike_width_half_max_df], axis=1)\n",
    "    spike_width_half_max_df.set_index('filename', inplace=True)\n",
    "    spike_width_half_max_df['metadata'] = metadata\n",
    "    spike_width_half_max_df.sort_index(inplace=True)\n",
    "    spike_width_half_max_df.to_excel(metadata + '_spike_width_half_max.xlsx')\n",
    "    return spike_width_half_max_df\n",
    "\n",
    "\n",
    "def determine_ss_input_resistance(path1, sample_rate, sweep_start, sweep_end, metadata, current_amplitude):\n",
    "    os.chdir(path1)\n",
    "    results_mean = []\n",
    "    results_cv = []\n",
    "    num_files_analyzed = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            file_results = []\n",
    "            for trace in traces:\n",
    "                for p in trace:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[17000:19500]  #selected for the first 10 seconds of each sweep\n",
    "                    voltages = (p['V']).flatten()\n",
    "                    voltages = voltages[17000:19500]  #selected for the first 10 seconds of each sweep\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    mean_voltages = np.mean(voltages)\n",
    "                    if mean_voltages is not None:\n",
    "                        file_results.append(mean_voltages)\n",
    "            if file_results:\n",
    "                mean_voltages = np.mean(file_results) # Average the results for the current file\n",
    "                input_resistance = np.absolute((mean_voltages / current_amplitude)) * 1000 # Calculate input resistance in MOhms\n",
    "                results_mean.append(input_resistance)\n",
    "                results_cv.append(np.absolute(np.std(file_results) / np.mean(file_results) * 100)) # Calculate the standard deviation of the results for the current file\n",
    "                num_files_analyzed.append(len(filename)) # Add the number of files analyzed for the current measure\n",
    "            else:\n",
    "                results_mean.append(np.nan)\n",
    "                results_cv.append(np.nan)\n",
    "                num_files_analyzed.append(0)\n",
    "    df = pd.DataFrame({'Input Resistance (MOhms)': results_mean, '%Variance': results_cv, 'Metadata': metadata})\n",
    "    df.to_excel(metadata + 'input_resistance.xlsx')\n",
    "    return df\n",
    "\n",
    "def determine_ss_average_membrane_potential(path1, sample_rate, sweep_start, sweep_end, metadata):\n",
    "    os.chdir(path1)\n",
    "    results_mean = []\n",
    "    results_cv = []\n",
    "    num_files_analyzed = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            file_results = []\n",
    "            for trace in traces:\n",
    "                for p in trace:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[10000:19500]  #selected for the first 10 seconds of each sweep\n",
    "                    voltages = (p['V']).flatten()\n",
    "                    voltages = voltages[10000:19500]  #selected for the first 10 seconds of each sweep\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    mean_voltages = np.mean(voltages)\n",
    "                    if mean_voltages is not None:\n",
    "                        file_results.append(mean_voltages)\n",
    "            if file_results:\n",
    "                results_mean.append(np.mean(file_results)) # Average the results for the current file\n",
    "                results_cv.append(np.absolute(np.std(file_results) / np.mean(file_results) * 100)) # Calculate the standard deviation of the results for the current file\n",
    "                num_files_analyzed.append(len(filename)) # Add the number of files analyzed for the current measure\n",
    "            else:\n",
    "                results_mean.append(np.nan)\n",
    "                results_std.append(np.nan)\n",
    "                num_files_analyzed.append(0)\n",
    "    df = pd.DataFrame({'Mean Voltages': results_mean, '%Variance': results_cv, 'Metadata': metadata})\n",
    "    df.to_excel(metadata + 'mean_voltages.xlsx')\n",
    "    return df\n",
    "\n",
    "def determine_ss_max_membrane_potential(path1, sample_rate, sweep_start, sweep_end, metadata):\n",
    "    os.chdir(path1)\n",
    "    results_mean = []\n",
    "    results_cv = []\n",
    "    num_files_analyzed = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            file_results = []\n",
    "            for trace in traces:\n",
    "                for p in trace:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[15000:19500]  #selected for the first 10 seconds of each sweep\n",
    "                    voltages = (p['V']).flatten()\n",
    "                    voltages = voltages[15000:19500]  #selected for the first 10 seconds of each sweep\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    mean_voltages = np.mean(voltages)\n",
    "                    if mean_voltages is not None:\n",
    "                        file_results.append(mean_voltages)\n",
    "            if file_results:\n",
    "                results_mean.append(np.max(file_results)) # Average the results for the current file\n",
    "                results_cv.append(np.absolute(np.std(file_results) / np.mean(file_results) * 100)) # Calculate the standard deviation of the results for the current file\n",
    "                num_files_analyzed.append(len(filename)) # Add the number of files analyzed for the current measure\n",
    "            else:\n",
    "                results_mean.append(np.nan)\n",
    "                results_std.append(np.nan)\n",
    "                num_files_analyzed.append(0)\n",
    "    df = pd.DataFrame({'Max Voltages': results_mean, '%Variance': results_cv, 'Metadata': metadata})\n",
    "    df.to_excel(metadata + 'mean_voltages.xlsx')\n",
    "    return df\n",
    "\n",
    "def mean_spike_voltages(path, sweep_start, sweep_end, detection_level, metadata):\n",
    "    spike_voltage_list = []\n",
    "    filename_list = []\n",
    "    os.chdir(path)\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "    for filename in file_names:\n",
    "        # check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            filename_list.append(filename)\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            table2 = []\n",
    "            for data in traces[sweep_start:sweep_end]:\n",
    "                for p in data:\n",
    "                    times = (p['T']) / 1000\n",
    "                    voltages = (p['V'])\n",
    "                    times -= times[0]\n",
    "                    dt = times[2] - times[1]\n",
    "                    detection_level = detection_level\n",
    "                    min_interval = 0.005\n",
    "                    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                    time_before = .025\n",
    "                    time_after = .015\n",
    "                    times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                    spike_voltages = []\n",
    "                    for i in range(0, len(spike_times)):\n",
    "                        if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                            spike_voltages.append(\n",
    "                                segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                    spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                    if len(spike_voltage_arrays) > 0:\n",
    "                        mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                        table2.append(mean_spike_voltages)\n",
    "            table3 = np.array(table2)\n",
    "            file_mean_spike_voltages = np.mean(table3, axis=0)\n",
    "            spike_voltage_list.append(file_mean_spike_voltages)\n",
    "    filename_list_df = pd.DataFrame(filename_list)\n",
    "    filename_list_df.columns = ['Filename']\n",
    "    global_spike_voltage_list_np = np.array(spike_voltage_list)\n",
    "    global_spike_voltages_df = pd.DataFrame(global_spike_voltage_list_np)\n",
    "    global_spike_voltages_df = pd.concat([filename_list_df, global_spike_voltages_df], axis=1)\n",
    "    global_spike_voltages_df.set_index('Filename', inplace=True)\n",
    "    global_spike_voltages_df.sort_index(inplace=True)\n",
    "    mean_spike_voltages = global_spike_voltages_df.mean(axis=0)\n",
    "    mean_spike_voltages_df = pd.DataFrame(mean_spike_voltages)\n",
    "    mean_spike_voltages_df.columns = ['Voltage (mV) ' + metadata]\n",
    "    #mean_spike_voltages_df.index = times_rel\n",
    "\n",
    "    return global_spike_voltages_df, mean_spike_voltages_df\n",
    "\n",
    "def calculate_dVdt(df, dt):\n",
    "    dVdt_df = pd.DataFrame(columns=df.columns)\n",
    "    for column in df.columns:\n",
    "        voltage = df[column].values\n",
    "        dVdt = (np.gradient(voltage, dt)) / 1000\n",
    "        dVdt_df[column] = dVdt\n",
    "    return dVdt_df\n",
    "\n",
    "def calculate_dVdt(df, dt):\n",
    "    # Convert the DataFrame to numeric, replacing non-numeric values with NaN\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    dVdt_df = pd.DataFrame(columns=df.columns)\n",
    "    for column in df.columns:\n",
    "        voltage = df[column].values\n",
    "        dVdt = (np.gradient(voltage, dt)) / 1000\n",
    "        dVdt_df[column] = dVdt\n",
    "    return dVdt_df\n",
    "\n",
    "def calculate_slope_and_mean_voltage_50ms_window(global_mean_traces, global_mean_isis, save_path, metadata):\n",
    "    os.chdir(save_path)\n",
    "    slopes = []\n",
    "    mean_voltages = []\n",
    "    min_voltages = []\n",
    "    for i, trace in enumerate(global_mean_traces):\n",
    "        if len(trace) == 0:\n",
    "            print(f\"Empty trace found at index {i}\")\n",
    "            continue\n",
    "        isi = global_mean_isis[i]\n",
    "        # print(isi)\n",
    "\n",
    "        # Scale the trace based on the actual length of the ISI\n",
    "        time = np.linspace(0, isi, len(trace))\n",
    "\n",
    "        slope_start_time = 10  # Start time of the window (e.g., 30% of ISI)\n",
    "        slope_end_time = 100  # End time of the window (e.g., 50ms window)\n",
    "\n",
    "        # Find the indices corresponding to the start and end times of the window\n",
    "        slope_start_idx = np.argmin(np.abs(time - slope_start_time))\n",
    "        slope_end_idx = np.argmin(np.abs(time - slope_end_time))\n",
    "        slope_slope_start_voltage_idx = int(0.3 * len(trace))  # find index of minimum voltage\n",
    "        end_idx_slope_slope = int(0.7 * len(trace))\n",
    "        # print('Start Index:', slope_start_idx)\n",
    "        # print('End Index:', slope_end_idx)\n",
    "\n",
    "        min_voltage_index = np.argmin(trace)\n",
    "        # print(min_voltage_index)\n",
    "        # print(slope_end_idx)\n",
    "        mean_voltage_end_idx = int(0.98 * len(trace))\n",
    "\n",
    "        # Calculate the slope using the scaled trace within the window\n",
    "        slope = np.gradient(trace[slope_slope_start_voltage_idx:end_idx_slope_slope],\n",
    "                            time[slope_slope_start_voltage_idx:end_idx_slope_slope]) * 1000\n",
    "\n",
    "        slopes.append(slope.mean())\n",
    "        mean_voltages.append(trace[slope_start_idx:slope_end_idx].mean())\n",
    "        min_voltages.append(trace[:].min())\n",
    "\n",
    "        plt.plot(time, trace)  # Plot the entire trace\n",
    "        plt.axvline(time[slope_start_idx], color='r', linestyle='--')  # Vertical line at start of window\n",
    "        plt.axvline(time[slope_end_idx], color='r', linestyle='--')  # Vertical line at end of window\n",
    "\n",
    "    slope_df = pd.DataFrame({'slope': slopes})\n",
    "    slope_df.to_excel(metadata + '_slopes_mAHP.xlsx')\n",
    "    mean_voltage_df = pd.DataFrame({'mean_voltage': mean_voltages})\n",
    "    mean_voltage_df.to_excel(metadata + '_mean_voltages_mAHP.xlsx')\n",
    "    min_voltage_df = pd.DataFrame({'min_voltage': min_voltages})\n",
    "    min_voltage_df.to_excel(metadata + '_min_voltages_mAHP.xlsx')\n",
    "\n",
    "    results_df = pd.concat([slope_df, mean_voltage_df, min_voltage_df], axis=1)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def calculate_spike_widths_and_thresholds(df, sampling_rate, dvdt_threshold, metadata, save_path):\n",
    "    os.chdir(save_path)\n",
    "    spike_widths = []\n",
    "    filenames = []\n",
    "    threshold = []\n",
    "    dt = 1 / sampling_rate\n",
    "    #the first thing we have to do is set the index to be the filename so that we dont have any strings in the df, but only if its there, so let write an if statement for it\n",
    "    if 'Filename' in df.columns:\n",
    "        df = df.set_index('Filename')\n",
    "    else:\n",
    "        pass\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract voltage trace and time values from the row\n",
    "        voltages = np.array(row.values)\n",
    "        times = np.arange(0, len(voltages)) / sampling_rate\n",
    "\n",
    "        # Find the first derivative of the voltage trace\n",
    "        dvdt = calculate_dVdt(df, dt)\n",
    "\n",
    "        # Find the start of the action potential (AP_begin_indices)\n",
    "        start_indices = np.where(dvdt >= dvdt_threshold)[0]\n",
    "\n",
    "        if len(start_indices) == 0:\n",
    "            continue\n",
    "        AP_begin_index = start_indices[0]\n",
    "        threshold.append(voltages[AP_begin_index])\n",
    "        # Find the peak of the action potential\n",
    "        peak_index = np.argmax(voltages)\n",
    "\n",
    "        # Calculate the half-max voltage value\n",
    "        half_max_voltage = (voltages[peak_index] + voltages[AP_begin_index]) / 2\n",
    "\n",
    "        # Find the indices corresponding to the time at spike start (AP_rise_indices) and time at spike end (AP_fall_indices)\n",
    "        AP_rise_indices = np.where(voltages >= half_max_voltage)[0]\n",
    "        AP_fall_indices = np.where(voltages <= half_max_voltage)[0]\n",
    "        AP_rise_index = AP_rise_indices[0]\n",
    "        AP_fall_index = AP_fall_indices[-1]\n",
    "\n",
    "        # Calculate the action potential duration at half width\n",
    "        spike_width = (times[AP_fall_index] - times[AP_rise_index]) * 100  # Convert to ms\n",
    "        spike_widths.append(spike_width)\n",
    "\n",
    "        # Extract the filename from the index\n",
    "        filenames.append(index)\n",
    "\n",
    "    spike_width_df = pd.DataFrame({'spike_width_half_max': spike_widths}, index=filenames)\n",
    "    threshold_df = pd.DataFrame({'threshold': threshold}, index=filenames)\n",
    "    final_df = spike_width_df.join(threshold_df)\n",
    "    final_df.to_excel(metadata + '_spike_widths_and_thresholds.xlsx')\n",
    "    return final_df\n",
    "\n",
    "def mean_thresholds_sd_cv(path, sweep_start, sweep_end, metadata):\n",
    "    filename_list = []\n",
    "    os.chdir(path)\n",
    "    v_threshold_list = []\n",
    "    v_std_list = []\n",
    "    cv_list = []\n",
    "\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            v_thresholds_all_traces = []\n",
    "\n",
    "            filename_list.append(filename)\n",
    "            print('Working on ' + filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=1000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "\n",
    "            for data in traces:\n",
    "                v_thresholds_trace = []\n",
    "\n",
    "                for p in data:\n",
    "                    times = (p['T']) / 1000\n",
    "                    times = times[:200000]\n",
    "                    voltages = (p['V'])\n",
    "                    voltages = voltages[:200000]\n",
    "                    voltages = voltages.flatten()\n",
    "                    if len(times) >= 3:\n",
    "                        times -= times[0]\n",
    "                    if len(times) >= 3:\n",
    "                        dt = times[2] - times[1]\n",
    "                    detection_level = -22\n",
    "                    min_interval = 0.0001\n",
    "                    spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                    isis = np.diff(spike_times)\n",
    "                    time_before = .018\n",
    "                    time_after = .015\n",
    "                    times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                    spike_voltages = []\n",
    "                    for i in range(0, len(spike_times)):\n",
    "                        if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                            spike_voltages.append(\n",
    "                                segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                    spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "\n",
    "                    for v_list in spike_voltage_arrays:\n",
    "                        dvdt_threshold = 10\n",
    "                        dvdt = find_slopes(v_list, dt)\n",
    "                        i = 1\n",
    "                        while dvdt[i] < dvdt_threshold:\n",
    "                            i += 1\n",
    "                        v0 = v_list[i - 1]\n",
    "                        v1 = v_list[i]\n",
    "                        dvdt0 = dvdt[i - 1]\n",
    "                        dvdt1 = dvdt[i]\n",
    "                        if dvdt1 - dvdt0 != 0:\n",
    "                            v_threshold = (v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0))\n",
    "                            v_thresholds_trace.append(v_threshold)\n",
    "\n",
    "                v_thresholds_all_traces.extend(v_thresholds_trace)\n",
    "\n",
    "            v_threshold_list.append(v_thresholds_all_traces)\n",
    "\n",
    "            # Calculate standard deviation and coefficient of variation for all spike waveforms in the file\n",
    "            v_threshold_array = np.array(v_thresholds_all_traces)\n",
    "\n",
    "            v_std = np.nanstd(v_threshold_array)\n",
    "            v_mean = np.nanmean(v_threshold_array)\n",
    "            cv = v_std / np.abs(v_mean) if v_mean != 0 else 0.0\n",
    "\n",
    "            v_std_list.append(v_std)\n",
    "            cv_list.append(cv)\n",
    "\n",
    "    # Calculate mean, standard deviation, and coefficient of variation for each file\n",
    "    v_std_array = np.array(v_std_list)\n",
    "    cv_array = np.array(cv_list)\n",
    "\n",
    "    v_std_df = pd.DataFrame(v_std_array, columns=['standard deviation'])\n",
    "    cv_df = pd.DataFrame(cv_array, columns=['coefficient of variation'])\n",
    "    filename_df = pd.DataFrame(filename_list, columns=['filename'])\n",
    "    results = pd.concat([v_std_df, cv_df, filename_df], axis=1)\n",
    "    results.set_index('filename', inplace=True)\n",
    "    results.sort_index(inplace=True)\n",
    "    results.to_excel(metadata + '_thresholds_sd_cv.xlsx')\n",
    "    return results\n",
    "\n",
    "\n",
    "def max_dVdt(df):\n",
    "    max_dVdt = []\n",
    "    for column in df.columns:\n",
    "        max_dVdt.append(df[column].max())\n",
    "    return max_dVdt\n",
    "\n",
    "\n",
    "def min_dVdt(df):\n",
    "    min_dVdt = []\n",
    "    for column in df.columns:\n",
    "        min_dVdt.append(df[column].min())\n",
    "    return min_dVdt\n",
    "\n",
    "\n",
    "def collect_frequency_and_cv(path1, sample_rate, sweep_start, sweep_end, time_start_s, time_end_s, metadata):\n",
    "    def one_by_isi(x):\n",
    "        return 1 / (x / 1000)\n",
    "\n",
    "    def calc_cv(interspike_intervals):\n",
    "        return np.std(interspike_intervals) / np.mean(interspike_intervals)\n",
    "\n",
    "    os.chdir(path1)\n",
    "    sampling_rate = sample_rate\n",
    "    time_start = time_start_s * sampling_rate\n",
    "    time_end = time_end_s * sampling_rate\n",
    "    global_mean_isi = []\n",
    "    isi_file_means = []\n",
    "    freq_file_means = []\n",
    "    global_mean_cv = []\n",
    "    filename_list = []\n",
    "    file_names = sorted(os.listdir())  # Sort the file names\n",
    "    for filename in file_names:\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            print('Working on ' + filename)\n",
    "            filename_list.append(filename)\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=0, stim_end=10000)\n",
    "            traces = traces[sweep_start:sweep_end]\n",
    "            if len(traces) > 0:\n",
    "                isi_list = []\n",
    "                cv_list = []\n",
    "                freq_list = []\n",
    "                for trace in traces:\n",
    "                    for p in trace:\n",
    "                        times = (p['T'])\n",
    "                        times = times[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        voltages = (p['V']).flatten()\n",
    "                        voltages = voltages[time_start:time_end]  #selected for the first 10 seconds of each sweep\n",
    "                        times -= times[0]\n",
    "                        dt = times[2] - times[1]\n",
    "                        detection_level = -10\n",
    "                        min_interval = 0.0001\n",
    "                        spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                        interspike_intervals = np.diff(spike_times)\n",
    "                        if len(interspike_intervals) > 0:\n",
    "                            avg_isi_1 = np.mean(interspike_intervals)\n",
    "                            avg_freq_1 = one_by_isi(avg_isi_1)\n",
    "                            cv_1 = calc_cv(interspike_intervals)\n",
    "                            isi_list.append(avg_isi_1)\n",
    "                            cv_list.append(cv_1)\n",
    "                            freq_list.append(avg_freq_1)\n",
    "                if len(isi_list) > 0:\n",
    "                    freq_means = np.mean(np.array(freq_list))\n",
    "                    freq_file_means.append(freq_means)\n",
    "                    isi_file_mean = np.mean(np.array(isi_list))\n",
    "                    isi_file_means.append(isi_file_mean)\n",
    "                    cv_file_mean = np.mean(np.array(cv_list))\n",
    "                    global_mean_isi.append(isi_file_mean)\n",
    "                    global_mean_cv.append(cv_file_mean)\n",
    "\n",
    "    filename_df = pd.DataFrame(filename_list)\n",
    "    filename_df.rename(columns={0: 'filename'}, inplace=True)\n",
    "    global_mean_isis = np.array(global_mean_isi)\n",
    "    global_mean_isi_df = pd.DataFrame(global_mean_isis)\n",
    "\n",
    "    freq_means_df = pd.DataFrame(freq_file_means)\n",
    "    freq_means_df.rename(columns={0: 'Frequency'}, inplace=True)\n",
    "    freq_means_df = pd.concat([filename_df, freq_means_df], axis=1)\n",
    "    freq_means_df.set_index('filename', inplace=True)\n",
    "    freq_means_df.sort_index(inplace=True)\n",
    "\n",
    "    global_mean_frequencies = global_mean_isi_df.applymap(one_by_isi)\n",
    "    global_mean_frequencies.rename(columns={0: 'Frequency'}, inplace=True)\n",
    "    global_mean_frequencies = pd.concat([filename_df, global_mean_frequencies], axis=1)\n",
    "    global_mean_frequencies.set_index('filename', inplace=True)\n",
    "    global_mean_frequencies.sort_index(inplace=True)\n",
    "\n",
    "    global_mean_cvs = np.array(global_mean_cv)\n",
    "    global_mean_cv_df = pd.DataFrame(global_mean_cvs)\n",
    "    global_mean_cv_df.rename(columns={0: 'CV_ISI'}, inplace=True)\n",
    "    global_mean_cv_df.set_index(global_mean_frequencies.index, inplace=True)\n",
    "\n",
    "    result_df = pd.concat([freq_means_df, global_mean_cv_df], axis=1)\n",
    "    result_df['Metadata'] = metadata\n",
    "    result_df.to_excel(metadata + '_frequency_and_cv.xlsx')\n",
    "    return result_df\n",
    "\n",
    "def ahc_analysis_step_time(path, time_start, time_end, metadata):\n",
    "    sample_rate = 20_000\n",
    "    start_time = int(time_start * sample_rate)\n",
    "    end_time = int(time_end * sample_rate)\n",
    "\n",
    "    def monoExp(x, m, t, b):\n",
    "        return m * np.exp(-t * x) + b\n",
    "\n",
    "    def biExp(x, m1, t1, m2, t2, b):\n",
    "        return m1 * np.exp(-t1 * x) + m2 * np.exp(-t2 * x) + b\n",
    "\n",
    "    ahc_append = []\n",
    "    ahc_auc_append = []\n",
    "\n",
    "    ahc_max_amp_append = []\n",
    "    auc_append = []\n",
    "    tau_append = []\n",
    "    ahc_max_amp_df_append = []\n",
    "    os.chdir(path)\n",
    "    file_names = sorted(os.listdir())\n",
    "    for file_name in file_names:\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][start_time:end_time]  #this is 16.101 to 16.213\n",
    "                    amps1 = data['A'][start_time:end_time]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][39200:39800])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "\n",
    "                    amps1_df = pd.DataFrame(amps1)  #we generated all the amps into a dataframe, check\n",
    "                    flat_times = np.ndarray.flatten(\n",
    "                        times1)  #we have all of the times into a flattened numpy array, check\n",
    "                    tabla.append(amps1_df)  #and we appended all of the amps into a dataframe\n",
    "                amps_concat = pd.concat(tabla,\n",
    "                                        axis=1)  #this is correct - we put all the amps for a given trace into a dataframe\n",
    "            #now we need to average those dataframes by row\n",
    "            averaged_sk_trace = amps_concat.mean(\n",
    "                axis=1)  #woohooo this is the averaged SK trace in a pd.DF, this is what we want to work with from here on out!\n",
    "            #print(np.argmax(averaged_sk_trace)) #this is the index of the max value of the averaged trace\n",
    "\n",
    "            flat_times = flat_times[\n",
    "                         np.argmax(averaged_sk_trace):]  #this is the times from the max value to the end of the trace\n",
    "            averaged_sk_trace = averaged_sk_trace[np.argmax(\n",
    "                averaged_sk_trace):]  #this is the averaged trace from the max value to the end of the trace\n",
    "\n",
    "            #Block of code of AHC Max Amp !\n",
    "            ahc_max_amp = pd.DataFrame.max(\n",
    "                averaged_sk_trace)  #max of the whole ahc, not just sk component, This is the value!!\n",
    "            #figure out to how put this in a format that can be read and concatenated\n",
    "            ahc_max_amp_array = np.array(ahc_max_amp, ndmin=2)  #these lines are new\n",
    "            ahc_max_amp_df = pd.DataFrame(ahc_max_amp_array)\n",
    "            ahc_max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "\n",
    "            ahc_append.append(ahc_max_amp_df)  #Here!\n",
    "            ahc_max_amp_concat_df = pd.concat(ahc_append)  #This is the variable for ahc\n",
    "\n",
    "            #Block of code for AHC AUC\n",
    "            averaged_sk_trace_as_np = averaged_sk_trace.to_numpy()\n",
    "            flattened_average_sk_trace = np.ndarray.flatten(averaged_sk_trace_as_np)\n",
    "            ahc_auc = np.trapz(flattened_average_sk_trace) / 1000  #this is the auc of the whole ahc\n",
    "            #bit of code to get the ahc auc into readable condition\n",
    "            ahc_auc_array = np.array(ahc_auc, ndmin=2)\n",
    "            ahc_auc_df = pd.DataFrame(ahc_auc_array)  #here we have the auc data in a dataframe\n",
    "            ahc_auc_df['file_name'] = file_name\n",
    "            ahc_auc_append.append(ahc_auc_df)\n",
    "            ahc_auc_concat_df = pd.concat(ahc_auc_append)  #this is the variable for auc\n",
    "            plt.plot(flat_times, averaged_sk_trace)  #check\n",
    "            plt.fill_between(flat_times, 0, flattened_average_sk_trace, color='gray', alpha=0.5, label='AUC')\n",
    "\n",
    "            #Block of code for kinetics\n",
    "            trace_for_kinetics = flattened_average_sk_trace[:]\n",
    "            times_rel = flat_times - flat_times[0]\n",
    "            times_for_kinetics = times_rel[:]\n",
    "            trace_for_kinetics_pd = pd.DataFrame(trace_for_kinetics)\n",
    "            #trace_for_kinetics_pd.to_excel(\"trace_for_kinetics_pd.xlsx\")\n",
    "            times_for_kinetics_pd = pd.DataFrame(times_for_kinetics)\n",
    "            #times_for_kinetics_pd.to_excel(\"times_for_kinetics_pd.xlsx\")\n",
    "\n",
    "            #fit the curve for inactivation tau\n",
    "            # p0 = [500, .001, 50]  #values near what we expect   #here\n",
    "            # params, cv = scipy.optimize.curve_fit(monoExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "            #                                       bounds=(-np.inf, np.inf),\n",
    "            #                                       maxfev=100000)  #here  #this fits the training curve with an r-squared of 0.97\n",
    "            # m, t, b = params  #here\n",
    "\n",
    "            # p0 = [500, .001, 500, .001, 50]  # initial parameter values\n",
    "            # params, cv = scipy.optimize.curve_fit(biExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "            #                                       bounds=(-np.inf, np.inf),\n",
    "            #                                       maxfev=100000000)  # fitting the curve\n",
    "            # m1, t1, m2, t2, b = params\n",
    "            #\n",
    "            # #plot results\n",
    "            # plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "            # plt.plot(times_for_kinetics, biExp(times_for_kinetics, m1, t1, m2, t2, b), '--', label=\"fitted\")\n",
    "            # plt.title(\"Fitted Biexponential Curve\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "            #m, t = params\n",
    "            sampleRate = 20_000  #hz\n",
    "            # tauSec = (1 / t) / sampleRate\n",
    "            # print(tauSec)\n",
    "    #\n",
    "    #         #determine quality of fit\n",
    "    #         squaredDiffs = np.square(trace_for_kinetics - monoExp(times_for_kinetics, m, t, b))  #here\n",
    "    #         squaredDiffsFromMean = np.square(trace_for_kinetics - np.mean(trace_for_kinetics))\n",
    "    #         rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "    #             squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "    #         #print(f\"R^2 = {rSquared}\")\n",
    "    #\n",
    "    # #plot results\n",
    "    # plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "    # plt.plot(times_for_kinetics, monoExp(times_for_kinetics, m, t, b), '--', label=\"fitted\")  #here\n",
    "    # plt.show()\n",
    "    # plt.title(\"Fitted Expotential Curve\")\n",
    "    #\n",
    "    #         #inspect the params\n",
    "    #         #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "    #         #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "    #         plt.show()\n",
    "    #         tau_flat_ms = tauSec * 1e4\n",
    "    #\n",
    "    #         #Bit of code to get tau into working order\n",
    "    #         if 0 <= tauSec*1e4 <= 300:\n",
    "    #             tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "    #             tau_df = pd.DataFrame(tau_array)\n",
    "    #             tau_df['file_name'] = file_name\n",
    "    #             tau_append.append(tau_df)\n",
    "    #             tau_concat_df = pd.concat(tau_append)   #this is the variable for tau\n",
    "    #         else:\n",
    "    #             print(file_name + ' is not within the range of 0-300ms')\n",
    "    #\n",
    "    #lets rename columns and export to excel for each of our metrics\n",
    "    ahc_max_amp_concat_df.rename(columns={0: 'Control AHC Max_Amplitude (pA)'}, inplace=True)\n",
    "    ahc_max_amp_concat_df.to_excel('ahc_max_amp ' + metadata + '.xlsx', index=False)\n",
    "\n",
    "    ahc_auc_concat_df.rename(columns={0: 'Control AHC AUC (pA*s)'}, inplace=True)\n",
    "    ahc_auc_concat_df.to_excel('ahc_auc ' + metadata + '.xlsx', index=False)\n",
    "    #\n",
    "    # tau_concat_df.rename(columns = {0:'Control Decay Tau (ms)'}, inplace=True)\n",
    "    # tau_concat_df.to_excel('ahc_tau ' + metadata + '.xlsx', index=False)\n",
    "    #\n",
    "    return display(ahc_max_amp_concat_df), display(ahc_auc_concat_df)  #, display(tau_concat_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d244a5b42913e3ae"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement nbgit (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for nbgit\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:03:58.219455Z",
     "start_time": "2024-01-11T19:03:56.103254Z"
    }
   },
   "id": "e66e5d7f992f5b5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2c4320ddfe01ec8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
